{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <span style=\"color:#0b486b\">FIT3162 MCS13 Code</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln8OFSFQMFwg"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">Imports</span>\n",
        "\n",
        "Library imports required for this file. \n",
        "- `os` - OS related functions (getting/setting current directory or filepath(s))\n",
        "- `zipfile` - to unzip files\n",
        "- `PIL.Image` -  Image related functions\n",
        "- `sklearn.model_selection.train_test_split` - A function to split up datasets\n",
        "- `tensorflow` - General Deep Learning purposes\n",
        "  - `keras`\n",
        "    - `layers, models, activations` - for Deep Learning Model creation\n",
        "- `matplotlib.pyplot` - for plotting graphs (if needed)\n",
        "- `numpy` - for array types (needed for tensorflow functions)\n",
        "- `BaseModel` - custom CNN Model class\n",
        "- `ResNetModel` - custom CNN Model class, with ResNet blocks, Batch Normalisation and Dropout Layers\n",
        "- `constants` - Constant values used throughout the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZHuTrv6o3ZB",
        "outputId": "59157c4a-b403-4ed1-9ee0-4ab1e212ca9b"
      },
      "outputs": [],
      "source": [
        "\"\"\" Code Header\n",
        "This file contains code thats trains and tests Models for Image Classification.\n",
        "Various Model architectures are used and tried. \n",
        "\n",
        "In order to make use of the dataset files, ensure that the download.py file under the RealWorldOccludedFaces-main directory is run already.\n",
        "\n",
        "Note: \n",
        "This jupyter file was obtained and altered from the Google Colab file from the MCS13 private Google Drive\n",
        "The file was created on 08/03/2024 and contained code cells for: \n",
        "- Image loading\n",
        "- Image resizing\n",
        "- Creation of some simple models (obtained and altered from GeminiAI and Microsoft Copilot)\n",
        "- Model training\n",
        "\n",
        "@author MCS13\n",
        "@version 1.1.0\n",
        "@since 29/03/2024\n",
        "@updated 30/03/2024\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================================================= #\n",
        "\n",
        "# Imports \n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, activations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from typing import List, Tuple\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "from BaseModel import BaseModel\n",
        "from ResNetModel import ResNetModel\n",
        "from constants import *\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1s2przVkFxF"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">Loading Datasets</span>\n",
        "\n",
        "***[RUN ONCE]*** Unzip and extract all files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E5o21XPKMns0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RUN ONCE: Unzipping the dataset zip file '"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def unzip_file(filepath: str):\n",
        "    \"\"\"Unzips a zip file\n",
        "    This code unzips a file and saves the content in the current directory.\n",
        "    This means the content will be under the same directory as the original zip file.\n",
        "\n",
        "    @since 1.0.0\n",
        "\n",
        "    @prerequisite: The input filepath must end with \".zip\"\n",
        "    @raise ValueError: When the input filepath does not end with \".zip\"\n",
        "\n",
        "    @param filepath(str): The file path to unzip. The file path must lead to a zip file\n",
        "    \"\"\"\n",
        "    # Pre-requisite check\n",
        "    if filepath[-4:] != \".zip\":\n",
        "        raise ValueError(\n",
        "            f\"The input filepath must end with '.zip'. Expected filepath = '<FileName>.zip', got filepath = '{filepath}'.\"\n",
        "        )\n",
        "\n",
        "    # Unzip the file\n",
        "    zip_obj = zipfile.ZipFile(file=filepath, mode=\"r\")\n",
        "    zip_obj.extractall(\"./\")\n",
        "    zip_obj.close()\n",
        "\n",
        "\n",
        "\"\"\"RUN ONCE: Unzipping the dataset zip file \"\"\"\n",
        "# dataset_zip_filepath = f\"{DATASET_NAME_MAIN}.zip\"\n",
        "# unzip_file(dataset_zip_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mEVeudOkJS9"
      },
      "source": [
        "Here, we start with the neutral faces dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6F0t2BgNNBf",
        "outputId": "50fd3ce8-96a2-4ab9-d949-4e311479043f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A few samples of people in the dataset: ['adrien_brody', 'alain_delon', 'alexander_zverev', 'al_pacino', 'amber_heard']\n",
            "Number of unique ids: 182\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f'A few samples of people in the dataset: {os.listdir(\"RealWorldOccludedFaces-main/images/neutral\")[:5]}'\n",
        ")\n",
        "print(\n",
        "    f'Number of unique ids: {len(os.listdir(\"RealWorldOccludedFaces-main/images/neutral\"))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8OYJn8Dircb"
      },
      "source": [
        "***[RUN ONCE]*** Resizes all images to a fixed shape of (64, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uw0A6wsPFteI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RUN ONCE: Resizing image dataset'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Main file paths\n",
        "main_path = f\"{DATASET_NAME_MAIN}/{NEUTRAL_DIR}\"\n",
        "main_dirs = os.listdir(main_path)\n",
        "num_classes = len(main_dirs)\n",
        "\n",
        "def resize_neutral_images(resized_shape: Tuple[int, int]):\n",
        "    \"\"\"Resizes all neutral images\n",
        "    The function takes all images in RealWorldOccludedFaces-main/images/neutral\n",
        "      and resizes them to 224x224 (determined by RESIZED_SHAPE constant)\n",
        "\n",
        "    The resized shapes are then saved in RealWorldOccludedFaces-resized/images/neutral\n",
        "\n",
        "    @param resized_shape(Tuple[int, int]): The shape to resize the shape to.\n",
        "    \"\"\"\n",
        "    # Creates the directory paths\n",
        "    resized_path = f\"{DATASET_NAME_RESIZED}_{resized_shape[0]}/{NEUTRAL_DIR}\"\n",
        "\n",
        "    for path in [os.path.join(resized_path, f) for f in os.listdir(main_path)]:\n",
        "        os.makedirs(f\"{path}\")\n",
        "    \n",
        "    # File paths of the neutral images\n",
        "    paths = [\n",
        "        os.path.join(resized_path, f)\n",
        "        for f in main_dirs\n",
        "    ]\n",
        "\n",
        "    # Resizing and saving all images\n",
        "    for name in os.listdir(main_path):\n",
        "        for img_name in os.listdir(f\"{main_path}/{name}\"):\n",
        "            Image.open(os.path.join(f\"{main_path}/{name}/\", img_name)).resize(\n",
        "                resized_shape\n",
        "            ).save(os.path.join(f\"{resized_path}/{name}/\", img_name))\n",
        "\n",
        "\n",
        "# Creating the directory paths before resizing the images\n",
        "# Needed before saving files\n",
        "\"\"\"RUN ONCE: Resizing image dataset\"\"\"\n",
        "# resize_neutral_images(RESIZED_SHAPES[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU1TlocxkOVD"
      },
      "source": [
        "Function to load the images into numpy arrays for model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i5gjYvWHbFRO"
      },
      "outputs": [],
      "source": [
        "def get_neutral_image_data(resized_shape: Tuple[int, int] = (64,64) ) -> (\n",
        "    Tuple[NDArray[List[List[int]]], NDArray[NDArray[List[List[int]]]]]\n",
        "):\n",
        "    \"\"\"Generates the dataset values (as ndarrays)\n",
        "    Produces the dataset values (x-values) and\n",
        "        the one-hot vector encoding of their labels (y-values)\n",
        "\n",
        "    The values are obtained from the neutral images\n",
        "\n",
        "    @rtype: Tuple[ndarray[List[List[int]]], ndarray[ndarray[List[List[uint8]]]]]\n",
        "    @return: x-values, y-values\n",
        "    \"\"\"\n",
        "    resized_path = f\"{DATASET_NAME_RESIZED}_{resized_shape[0]}/{NEUTRAL_DIR}\"\n",
        "\n",
        "    # File paths of the neutral images\n",
        "    paths = [\n",
        "        os.path.join(resized_path, f)\n",
        "        for f in main_dirs\n",
        "    ]\n",
        "\n",
        "    # Return the dataset values\n",
        "    return np.array(\n",
        "        [\n",
        "            np.array(Image.open(img), \"uint8\")\n",
        "            for path in paths\n",
        "            for img in [os.path.join(path, f) for f in os.listdir(path)]\n",
        "        ]\n",
        "    ), np.array(\n",
        "        [\n",
        "            [int(bit == unique_id) for bit in range(len(os.listdir(main_path)))]\n",
        "            for unique_id, path in enumerate(paths)\n",
        "            for _ in [os.path.join(path, f) for f in os.listdir(path)]\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "6KmFkJuVUx8c",
        "outputId": "0204bd12-2791-4e54-a460-5daf8d723f72"
      },
      "outputs": [],
      "source": [
        "# Takes a while...\n",
        "faces, ids = get_neutral_image_data(RESIZED_SHAPES[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GIjGOzuIbVua"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of face images: 5927\n",
            "Number of ids (must match above): 5927\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of face images: {len(faces)}\")  # size of neutral faces dataset\n",
        "print(f\"Number of ids (must match above): {len(ids)}\")  # size of the array of face ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKl1TbNVeJV"
      },
      "source": [
        "Splitting the dataset into training, validation and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M11ZyDR1VSlC"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and testing datasets\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    faces, ids, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Splitting the training dataset into training and validation datasets\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train, y_train, test_size=0.1, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "--- \n",
        "\n",
        "## <span style=\"color:#0b486b\">Modelling time!</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BaseModel(\n",
        "    name=\"Basic Model\",\n",
        "    input_width=RESIZED_SHAPES[0][0],\n",
        "    input_height=RESIZED_SHAPES[0][1],\n",
        "    depth=DEPTH,\n",
        "    num_classes=num_classes,\n",
        "    activation_func=RELU,\n",
        "    optimiser=ADAM_OPT,\n",
        ")\n",
        "\n",
        "model.build_cnn()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPEZMHd0WJWM"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lPSgS0XWL3n"
      },
      "outputs": [],
      "source": [
        "evaluation_results = model.compute_accuracy(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">ResNet Upgrade!</span>\n",
        "\n",
        "Now, we upgrade the model by implementing **ResNet blocks**, **Dropout layers** and **Batch Normalisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 96, 96, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 96, 96, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 96, 96, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 96, 96, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 96, 96, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 96, 96, 32)        0         \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 48, 48, 32)       0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 48, 48, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 48, 48, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 48, 48, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 48, 48, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 48, 48, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 48, 48, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 48, 48, 64)        0         \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 24, 24, 64)       0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 24, 24, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 24, 24, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 24, 24, 128)       0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 24, 24, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 24, 24, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 24, 24, 128)       0         \n",
            "                                                                 \n",
            " average_pooling2d_2 (Averag  (None, 12, 12, 128)      0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18432)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 182)               3354806   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,643,606\n",
            "Trainable params: 3,642,710\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "resnet_model = ResNetModel(\n",
        "    name=\"Basic Model\",\n",
        "    input_width=RESIZED_SHAPES[0][0],\n",
        "    input_height=RESIZED_SHAPES[0][1],\n",
        "    depth=DEPTH,\n",
        "    num_classes=num_classes,\n",
        "    activation_func=RELU,\n",
        "    optimiser=ADAM_OPT,\n",
        ")\n",
        "\n",
        "resnet_model.build_cnn()\n",
        "resnet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "134/134 [==============================] - 91s 670ms/step - loss: 5.1398 - accuracy: 0.0424\n",
            "Epoch 2/20\n",
            "134/134 [==============================] - 97s 725ms/step - loss: 3.6900 - accuracy: 0.2030\n",
            "Epoch 3/20\n",
            "134/134 [==============================] - 115s 857ms/step - loss: 2.6365 - accuracy: 0.4027\n",
            "Epoch 4/20\n",
            "134/134 [==============================] - 240s 2s/step - loss: 1.8318 - accuracy: 0.5881\n",
            "Epoch 5/20\n",
            "134/134 [==============================] - 116s 865ms/step - loss: 1.2875 - accuracy: 0.7211\n",
            "Epoch 6/20\n",
            "134/134 [==============================] - 96s 714ms/step - loss: 0.8952 - accuracy: 0.8279\n",
            "Epoch 7/20\n",
            "134/134 [==============================] - 95s 706ms/step - loss: 0.6256 - accuracy: 0.8915\n",
            "Epoch 8/20\n",
            "134/134 [==============================] - 94s 704ms/step - loss: 0.4419 - accuracy: 0.9372\n",
            "Epoch 9/20\n",
            "134/134 [==============================] - 94s 703ms/step - loss: 0.3013 - accuracy: 0.9674\n",
            "Epoch 10/20\n",
            "134/134 [==============================] - 95s 706ms/step - loss: 0.2272 - accuracy: 0.9798\n",
            "Epoch 11/20\n",
            "134/134 [==============================] - 130s 974ms/step - loss: 0.1711 - accuracy: 0.9880\n",
            "Epoch 12/20\n",
            "134/134 [==============================] - 235s 2s/step - loss: 0.1279 - accuracy: 0.9925\n",
            "Epoch 13/20\n",
            "134/134 [==============================] - 228s 2s/step - loss: 0.0973 - accuracy: 0.9939\n",
            "Epoch 14/20\n",
            "134/134 [==============================] - 166s 1s/step - loss: 0.0881 - accuracy: 0.9948\n",
            "Epoch 15/20\n",
            "134/134 [==============================] - 97s 722ms/step - loss: 0.0745 - accuracy: 0.9958\n",
            "Epoch 16/20\n",
            "134/134 [==============================] - 99s 736ms/step - loss: 0.0640 - accuracy: 0.9965\n",
            "Epoch 17/20\n",
            "134/134 [==============================] - 98s 733ms/step - loss: 0.0488 - accuracy: 0.9972\n",
            "Epoch 18/20\n",
            "134/134 [==============================] - 131s 980ms/step - loss: 0.0484 - accuracy: 0.9972\n",
            "Epoch 19/20\n",
            "134/134 [==============================] - 94s 705ms/step - loss: 0.0389 - accuracy: 0.9977\n",
            "Epoch 20/20\n",
            "134/134 [==============================] - 95s 708ms/step - loss: 0.0332 - accuracy: 0.9981\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1a292ed9490>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "resnet_model.fit(x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38/38 [==============================] - 7s 186ms/step - loss: 2.6314 - accuracy: 0.4983\n",
            "loss: 2.631420135498047\n",
            "accuracy: 0.49831366539001465\n"
          ]
        }
      ],
      "source": [
        "evaluation_results = resnet_model.compute_accuracy(x_test=x_test, y_test=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Version Overview**\n",
        "\n",
        "1.0.0\n",
        "- Copied file from jupyter notebook in MCS13 private Google Drive\n",
        "- Imported new BaseModel and constants files\n",
        "- Did basic training using BaseModel \n",
        "\n",
        "1.1.0\n",
        "- Imported ResNetModel"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

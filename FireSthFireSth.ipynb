{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <span style=\"color:#0b486b\">FIT3162 MCS13 Code</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln8OFSFQMFwg"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">Imports</span>\n",
        "\n",
        "Library imports required for this file. \n",
        "- `os` - OS related functions (getting/setting current directory or filepath(s))\n",
        "- `zipfile` - to unzip files\n",
        "- `PIL.Image` -  Image related functions\n",
        "- `sklearn.model_selection.train_test_split` - A function to split up datasets\n",
        "- `tensorflow` - General Deep Learning purposes\n",
        "  - `keras`\n",
        "    - `layers, models, activations` - for Deep Learning Model creation\n",
        "- `matplotlib.pyplot` - for plotting graphs (if needed)\n",
        "- `numpy` - for array types (needed for tensorflow functions)\n",
        "- `BaseModel` - custom CNN Model class\n",
        "- `ResNetModel` - custom CNN Model class, with ResNet blocks, Batch Normalisation and Dropout Layers\n",
        "- `SkipConnModel` - custom CNN Model class, with ResNetModel features and Skip Connection. Some Skip Connection with Conv2D layers too.\n",
        "- `constants` - Constant values used throughout the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZHuTrv6o3ZB",
        "outputId": "59157c4a-b403-4ed1-9ee0-4ab1e212ca9b"
      },
      "outputs": [],
      "source": [
        "\"\"\" Code Header\n",
        "This file contains code thats trains and tests Models for Image Classification.\n",
        "Various Model architectures are used and tried. \n",
        "\n",
        "In order to make use of the dataset files, ensure that the download.py file under the RealWorldOccludedFaces-main directory is run already.\n",
        "\n",
        "Note: \n",
        "This jupyter file was obtained and altered from the Google Colab file from the MCS13 private Google Drive\n",
        "The file was created on 08/03/2024 and contained code cells for: \n",
        "- Image loading\n",
        "- Image resizing\n",
        "- Creation of some simple models (obtained and altered from GeminiAI and Microsoft Copilot)\n",
        "- Model training\n",
        "\n",
        "@author MCS13\n",
        "@version 1.3.0\n",
        "@since 29/03/2024\n",
        "@updated 05/04/2024\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================================================= #\n",
        "\n",
        "# Imports \n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, activations\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from typing import List, Tuple\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "from BaseModel import BaseModel\n",
        "from ResNetModel import ResNetModel\n",
        "from SkipConnModel import SkipConnModel\n",
        "from constants import *\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1s2przVkFxF"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">Loading Datasets</span>\n",
        "\n",
        "***[RUN ONCE]*** Unzip and extract all files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5o21XPKMns0"
      },
      "outputs": [],
      "source": [
        "def unzip_file(filepath: str):\n",
        "    \"\"\"Unzips a zip file\n",
        "    This code unzips a file and saves the content in the current directory.\n",
        "    This means the content will be under the same directory as the original zip file.\n",
        "\n",
        "    @since 1.0.0\n",
        "\n",
        "    @prerequisite: The input filepath must end with \".zip\"\n",
        "    @raise ValueError: When the input filepath does not end with \".zip\"\n",
        "\n",
        "    @param filepath(str): The file path to unzip. The file path must lead to a zip file\n",
        "    \"\"\"\n",
        "    # Pre-requisite check\n",
        "    if filepath[-4:] != \".zip\":\n",
        "        raise ValueError(\n",
        "            f\"The input filepath must end with '.zip'. Expected filepath = '<FileName>.zip', got filepath = '{filepath}'.\"\n",
        "        )\n",
        "\n",
        "    # Unzip the file\n",
        "    zip_obj = zipfile.ZipFile(file=filepath, mode=\"r\")\n",
        "    zip_obj.extractall(\"./\")\n",
        "    zip_obj.close()\n",
        "\n",
        "\n",
        "\"\"\"RUN ONCE: Unzipping the dataset zip file \"\"\"\n",
        "# dataset_zip_filepath = f\"{DATASET_NAME_MAIN}.zip\"\n",
        "# unzip_file(dataset_zip_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mEVeudOkJS9"
      },
      "source": [
        "Here, we start with the neutral faces dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6F0t2BgNNBf",
        "outputId": "50fd3ce8-96a2-4ab9-d949-4e311479043f"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f'A few samples of people in the dataset: {os.listdir(\"RealWorldOccludedFaces-main/images/neutral\")[:5]}'\n",
        ")\n",
        "print(\n",
        "    f'Number of unique ids: {len(os.listdir(\"RealWorldOccludedFaces-main/images/neutral\"))}'\n",
        ")\n",
        "print(\n",
        "    f'Some important constants:\\nRESIZED_SHAPES: {RESIZED_SHAPES}\\nWIDTH: {WIDTH}\\nHEIGHT: {HEIGHT}\\n'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8OYJn8Dircb"
      },
      "source": [
        "***[RUN ONCE]*** Resizes all images to a fixed shape of (64, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw0A6wsPFteI"
      },
      "outputs": [],
      "source": [
        "# Main file paths\n",
        "main_path = f\"{DATASET_NAME_MAIN}/{NEUTRAL_DIR}\"\n",
        "main_dirs = os.listdir(main_path)\n",
        "num_classes = len(main_dirs)\n",
        "\n",
        "def resize_neutral_images(resized_shape: Tuple[int, int]):\n",
        "    \"\"\"Resizes all neutral images\n",
        "    The function takes all images in RealWorldOccludedFaces-main/images/neutral\n",
        "      and resizes them to 224x224 (determined by RESIZED_SHAPE constant)\n",
        "\n",
        "    The resized shapes are then saved in RealWorldOccludedFaces-resized/images/neutral\n",
        "\n",
        "    @param resized_shape(Tuple[int, int]): The shape to resize the shape to.\n",
        "    \"\"\"\n",
        "    # Creates the directory paths\n",
        "    resized_path = f\"{DATASET_NAME_RESIZED}_{resized_shape[0]}/{NEUTRAL_DIR}\"\n",
        "\n",
        "    for path in [os.path.join(resized_path, f) for f in os.listdir(main_path)]:\n",
        "        os.makedirs(f\"{path}\")\n",
        "    \n",
        "    # File paths of the neutral images\n",
        "    paths = [\n",
        "        os.path.join(resized_path, f)\n",
        "        for f in main_dirs\n",
        "    ]\n",
        "\n",
        "    # Resizing and saving all images\n",
        "    for name in os.listdir(main_path):\n",
        "        for img_name in os.listdir(f\"{main_path}/{name}\"):\n",
        "            Image.open(os.path.join(f\"{main_path}/{name}/\", img_name)).resize(\n",
        "                resized_shape\n",
        "            ).save(os.path.join(f\"{resized_path}/{name}/\", img_name))\n",
        "\n",
        "\n",
        "# Creating the directory paths before resizing the images\n",
        "# Needed before saving files\n",
        "\"\"\"RUN ONCE: Resizing image dataset\"\"\"\n",
        "# resize_neutral_images(RESIZED_SHAPES[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU1TlocxkOVD"
      },
      "source": [
        "Function to load the images into numpy arrays for model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5gjYvWHbFRO"
      },
      "outputs": [],
      "source": [
        "def get_neutral_image_data(\n",
        "    resized_shape: Tuple[int, int] = (64, 64)\n",
        ") -> Tuple[NDArray[List[List[int]]], NDArray[NDArray[List[List[int]]]]]:\n",
        "    \"\"\"Generates the dataset values (as ndarrays)\n",
        "    Produces the dataset values (x-values) and\n",
        "        the one-hot vector encoding of their labels (y-values)\n",
        "\n",
        "    The values are obtained from the neutral images\n",
        "\n",
        "    @rtype: Tuple[ndarray[List[List[int]]], ndarray[ndarray[List[List[uint8]]]]]\n",
        "    @return: x-values, y-values\n",
        "    \"\"\"\n",
        "    resized_path = f\"{DATASET_NAME_RESIZED}_{resized_shape[0]}/{NEUTRAL_DIR}\"\n",
        "\n",
        "    # File paths of the neutral images\n",
        "    paths = [os.path.join(resized_path, f) for f in main_dirs]\n",
        "\n",
        "    faces = []\n",
        "    ids = []\n",
        "\n",
        "    for unique_id, path in enumerate(paths):\n",
        "        for img in [os.path.join(path, f) for f in os.listdir(path)]:\n",
        "            image = np.array(Image.open(img), \"uint8\")\n",
        "\n",
        "            # Translate the image laterally and vertically by a random amount.\n",
        "            translated_image_lateral = np.roll(image, np.random.randint(-5, 5), axis=1)\n",
        "            translated_image_vertical = np.roll(image, np.random.randint(-5, 5), axis=0)\n",
        "\n",
        "            # Add the original, laterally translated, and vertically translated images to the list.\n",
        "            faces.append(image)\n",
        "            faces.append(translated_image_lateral)\n",
        "            faces.append(translated_image_vertical)\n",
        "\n",
        "            # Add the corresponding labels to the list.\n",
        "            id = [int(bit == unique_id) for bit in range(len(os.listdir(main_path)))]\n",
        "            ids.append(id)\n",
        "            ids.append(id)\n",
        "            ids.append(id)\n",
        "\n",
        "    # Return the dataset values\n",
        "    return np.array(faces), np.array(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "6KmFkJuVUx8c",
        "outputId": "0204bd12-2791-4e54-a460-5daf8d723f72"
      },
      "outputs": [],
      "source": [
        "# Takes a while...\n",
        "faces, ids = get_neutral_image_data(RESIZED_SHAPES[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIjGOzuIbVua"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of face images: {len(faces)}\")  # size of neutral faces dataset\n",
        "print(f\"Number of ids (must match above): {len(ids)}\")  # size of the array of face ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKl1TbNVeJV"
      },
      "source": [
        "Splitting the dataset into training, validation and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M11ZyDR1VSlC"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and testing datasets\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    faces, ids, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Splitting the training dataset into training and validation datasets\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train, y_train, test_size=0.1, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "--- \n",
        "\n",
        "## <span style=\"color:#0b486b\">Modelling time!</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BaseModel(\n",
        "    name=\"Basic Model\",\n",
        "    input_width=RESIZED_SHAPES[0][0],\n",
        "    input_height=RESIZED_SHAPES[0][1],\n",
        "    depth=DEPTH,\n",
        "    num_classes=num_classes,\n",
        "    activation_func=RELU,\n",
        "    optimiser=ADAM_OPT,\n",
        ")\n",
        "\n",
        "model.build_cnn()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Commented out since we don't use this model anymore\n",
        "# model.fit(x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPEZMHd0WJWM"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lPSgS0XWL3n"
      },
      "outputs": [],
      "source": [
        "# Commented out since we don't use this model anymore\n",
        "# evaluation_results = model.compute_accuracy(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">ResNet Upgrade!</span>\n",
        "\n",
        "Now, we upgrade the model by implementing **ResNet blocks**, **Dropout layers** and **Batch Normalisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_model = ResNetModel(\n",
        "    name=\"ResNet Model\",\n",
        "    input_width=RESIZED_SHAPES[0][0],\n",
        "    input_height=RESIZED_SHAPES[0][1],\n",
        "    depth=DEPTH,\n",
        "    num_classes=num_classes,\n",
        "    activation_func=RELU,\n",
        "    optimiser=ADAM_OPT,\n",
        ")\n",
        "\n",
        "resnet_model.build_cnn()\n",
        "resnet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Commented out since we don't use this model anymore\n",
        "# resnet_model.fit(x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Commented out since we don't use this model anymore\n",
        "# evaluation_results = resnet_model.compute_accuracy(x_test=x_test, y_test=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">SkipConnection Upgrade!</span>\n",
        "\n",
        "Now, we upgrade the model by implementing **ResNet blocks**, **Dropout layers** and **Batch Normalisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "skip_conn_model = SkipConnModel(\n",
        "    name=\"ResNet Model\",\n",
        "    input_width=WIDTH,\n",
        "    input_height=HEIGHT,\n",
        "    depth=DEPTH,\n",
        "    num_classes=num_classes,\n",
        "    num_epochs = 30,\n",
        "    activation_func=RELU,\n",
        "    optimiser=ADAM_OPT,\n",
        ")\n",
        "\n",
        "skip_conn_model.build_cnn()\n",
        "skip_conn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stops early based on the validation loss\n",
        "# When running with GPU, could use higher patience level (cuz can afford to compute more hahah)\n",
        "# min_delta parameter is used to determine how much leeway to give for change of validation loss\n",
        "#       default is 0\n",
        "early_stopping = EarlyStopping(patience=5, monitor=\"val_loss\", mode=\"min\", min_delta=0)\n",
        "\n",
        "# Saves the best model based on validation loss\n",
        "val_loss_checkpoint = ModelCheckpoint(\n",
        "    os.path.join(\"./ckpts\", \"best_val\"),\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Saves the best model based on validation accuracy\n",
        "val_acc_checkpoint = ModelCheckpoint(\n",
        "    os.path.join(\"./ckpts\", \"best_acc\"),\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "skip_conn_model.fit(\n",
        "    x_train=x_train,\n",
        "    y_train=y_train,\n",
        "    x_val=x_val,\n",
        "    y_val=y_val,\n",
        "    callbacks=[early_stopping, val_loss_checkpoint, val_acc_checkpoint],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_results = skip_conn_model.compute_accuracy(x_test=x_test, y_test=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Version Overview**\n",
        "\n",
        "1.0.0\n",
        "- Copied file from jupyter notebook in MCS13 private Google Drive\n",
        "- Imported new BaseModel and constants files\n",
        "- Did basic training using BaseModel \n",
        "\n",
        "1.1.0\n",
        "- Imported ResNetModel\n",
        "- Included verbose during training in BaseModel class\n",
        "\n",
        "1.2.0\n",
        "- Introduced Skip Connection :D\n",
        "- Included callbacks parameter for .fit() function in BaseModel class\n",
        "\n",
        "1.3.0\n",
        "- Added data augmentation during loading dataset \n",
        "  - Currently only horizontal and vertical shifts\n",
        "  - Potential augmentations:\n",
        "    - Rotation\n",
        "    - Adding noise"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

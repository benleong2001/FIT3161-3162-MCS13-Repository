{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <span style=\"color:#0b486b\">FIT3162 MCS13 Code</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln8OFSFQMFwg"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">Imports</span>\n",
        "\n",
        "Library imports required for this file. \n",
        "- `os` - OS related functions (getting/setting current directory or filepath(s))\n",
        "- `zipfile` - to unzip files\n",
        "- `PIL.Image` -  Image related functions\n",
        "- `sklearn.model_selection.train_test_split` - A function to split up datasets\n",
        "- `tensorflow` - General Deep Learning purposes\n",
        "  - `keras`\n",
        "    - `layers, models, activations` - for Deep Learning Model creation\n",
        "- `matplotlib.pyplot` - for plotting graphs (if needed)\n",
        "- `numpy` - for array types (needed for tensorflow functions)\n",
        "- `BaseModel` - custom CNN Model class\n",
        "- `ResNetModel` - custom CNN Model class, with ResNet blocks, Batch Normalisation and Dropout Layers\n",
        "- `constants` - Constant values used throughout the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZHuTrv6o3ZB",
        "outputId": "59157c4a-b403-4ed1-9ee0-4ab1e212ca9b"
      },
      "outputs": [],
      "source": [
        "\"\"\" Code Header\n",
        "This file contains code thats trains and tests Models for Image Classification.\n",
        "Various Model architectures are used and tried. \n",
        "\n",
        "In order to make use of the dataset files, ensure that the download.py file under the RealWorldOccludedFaces-main directory is run already.\n",
        "\n",
        "Note: \n",
        "This jupyter file was obtained and altered from the Google Colab file from the MCS13 private Google Drive\n",
        "The file was created on 08/03/2024 and contained code cells for: \n",
        "- Image loading\n",
        "- Image resizing\n",
        "- Creation of some simple models (obtained and altered from GeminiAI and Microsoft Copilot)\n",
        "- Model training\n",
        "\n",
        "@author MCS13\n",
        "@version 1.1.0\n",
        "@since 29/03/2024\n",
        "@updated 30/03/2024\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================================================= #\n",
        "\n",
        "# Imports \n",
        "import os\n",
        "import zipfile\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, activations\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from typing import List, Tuple\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "from BaseModel import BaseModel\n",
        "from ResNetModel import ResNetModel\n",
        "from SkipConnModel import SkipConnModel\n",
        "from constants import *\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1s2przVkFxF"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">Loading Datasets</span>\n",
        "\n",
        "***[RUN ONCE]*** Unzip and extract all files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E5o21XPKMns0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RUN ONCE: Unzipping the dataset zip file '"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def unzip_file(filepath: str):\n",
        "    \"\"\"Unzips a zip file\n",
        "    This code unzips a file and saves the content in the current directory.\n",
        "    This means the content will be under the same directory as the original zip file.\n",
        "\n",
        "    @since 1.0.0\n",
        "\n",
        "    @prerequisite: The input filepath must end with \".zip\"\n",
        "    @raise ValueError: When the input filepath does not end with \".zip\"\n",
        "\n",
        "    @param filepath(str): The file path to unzip. The file path must lead to a zip file\n",
        "    \"\"\"\n",
        "    # Pre-requisite check\n",
        "    if filepath[-4:] != \".zip\":\n",
        "        raise ValueError(\n",
        "            f\"The input filepath must end with '.zip'. Expected filepath = '<FileName>.zip', got filepath = '{filepath}'.\"\n",
        "        )\n",
        "\n",
        "    # Unzip the file\n",
        "    zip_obj = zipfile.ZipFile(file=filepath, mode=\"r\")\n",
        "    zip_obj.extractall(\"./\")\n",
        "    zip_obj.close()\n",
        "\n",
        "\n",
        "\"\"\"RUN ONCE: Unzipping the dataset zip file \"\"\"\n",
        "# dataset_zip_filepath = f\"{DATASET_NAME_MAIN}.zip\"\n",
        "# unzip_file(dataset_zip_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mEVeudOkJS9"
      },
      "source": [
        "Here, we start with the neutral faces dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6F0t2BgNNBf",
        "outputId": "50fd3ce8-96a2-4ab9-d949-4e311479043f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A few samples of people in the dataset: ['adrien_brody', 'alain_delon', 'alexander_zverev', 'al_pacino', 'amber_heard']\n",
            "Number of unique ids: 182\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f'A few samples of people in the dataset: {os.listdir(\"RealWorldOccludedFaces-main/images/neutral\")[:5]}'\n",
        ")\n",
        "print(\n",
        "    f'Number of unique ids: {len(os.listdir(\"RealWorldOccludedFaces-main/images/neutral\"))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8OYJn8Dircb"
      },
      "source": [
        "***[RUN ONCE]*** Resizes all images to a fixed shape of (64, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uw0A6wsPFteI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'RUN ONCE: Resizing image dataset'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Main file paths\n",
        "main_path = f\"{DATASET_NAME_MAIN}/{NEUTRAL_DIR}\"\n",
        "main_dirs = os.listdir(main_path)\n",
        "num_classes = len(main_dirs)\n",
        "\n",
        "def resize_neutral_images(resized_shape: Tuple[int, int]):\n",
        "    \"\"\"Resizes all neutral images\n",
        "    The function takes all images in RealWorldOccludedFaces-main/images/neutral\n",
        "      and resizes them to 224x224 (determined by RESIZED_SHAPE constant)\n",
        "\n",
        "    The resized shapes are then saved in RealWorldOccludedFaces-resized/images/neutral\n",
        "\n",
        "    @param resized_shape(Tuple[int, int]): The shape to resize the shape to.\n",
        "    \"\"\"\n",
        "    # Creates the directory paths\n",
        "    resized_path = f\"{DATASET_NAME_RESIZED}_{resized_shape[0]}/{NEUTRAL_DIR}\"\n",
        "\n",
        "    for path in [os.path.join(resized_path, f) for f in os.listdir(main_path)]:\n",
        "        os.makedirs(f\"{path}\")\n",
        "    \n",
        "    # File paths of the neutral images\n",
        "    paths = [\n",
        "        os.path.join(resized_path, f)\n",
        "        for f in main_dirs\n",
        "    ]\n",
        "\n",
        "    # Resizing and saving all images\n",
        "    for name in os.listdir(main_path):\n",
        "        for img_name in os.listdir(f\"{main_path}/{name}\"):\n",
        "            Image.open(os.path.join(f\"{main_path}/{name}/\", img_name)).resize(\n",
        "                resized_shape\n",
        "            ).save(os.path.join(f\"{resized_path}/{name}/\", img_name))\n",
        "\n",
        "\n",
        "# Creating the directory paths before resizing the images\n",
        "# Needed before saving files\n",
        "\"\"\"RUN ONCE: Resizing image dataset\"\"\"\n",
        "# resize_neutral_images(RESIZED_SHAPES[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU1TlocxkOVD"
      },
      "source": [
        "Function to load the images into numpy arrays for model input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i5gjYvWHbFRO"
      },
      "outputs": [],
      "source": [
        "def get_neutral_image_data(resized_shape: Tuple[int, int] = (64,64) ) -> (\n",
        "    Tuple[NDArray[List[List[int]]], NDArray[NDArray[List[List[int]]]]]\n",
        "):\n",
        "    \"\"\"Generates the dataset values (as ndarrays)\n",
        "    Produces the dataset values (x-values) and\n",
        "        the one-hot vector encoding of their labels (y-values)\n",
        "\n",
        "    The values are obtained from the neutral images\n",
        "\n",
        "    @rtype: Tuple[ndarray[List[List[int]]], ndarray[ndarray[List[List[uint8]]]]]\n",
        "    @return: x-values, y-values\n",
        "    \"\"\"\n",
        "    resized_path = f\"{DATASET_NAME_RESIZED}_{resized_shape[0]}/{NEUTRAL_DIR}\"\n",
        "\n",
        "    # File paths of the neutral images\n",
        "    paths = [\n",
        "        os.path.join(resized_path, f)\n",
        "        for f in main_dirs\n",
        "    ]\n",
        "\n",
        "    # Return the dataset values\n",
        "    return np.array(\n",
        "        [\n",
        "            np.array(Image.open(img), \"uint8\")\n",
        "            for path in paths\n",
        "            for img in [os.path.join(path, f) for f in os.listdir(path)]\n",
        "        ]\n",
        "    ), np.array(\n",
        "        [\n",
        "            [int(bit == unique_id) for bit in range(len(os.listdir(main_path)))]\n",
        "            for unique_id, path in enumerate(paths)\n",
        "            for _ in [os.path.join(path, f) for f in os.listdir(path)]\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "6KmFkJuVUx8c",
        "outputId": "0204bd12-2791-4e54-a460-5daf8d723f72"
      },
      "outputs": [],
      "source": [
        "# Takes a while...\n",
        "faces, ids = get_neutral_image_data(RESIZED_SHAPES[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GIjGOzuIbVua"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of face images: 5927\n",
            "Number of ids (must match above): 5927\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of face images: {len(faces)}\")  # size of neutral faces dataset\n",
        "print(f\"Number of ids (must match above): {len(ids)}\")  # size of the array of face ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuKl1TbNVeJV"
      },
      "source": [
        "Splitting the dataset into training, validation and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M11ZyDR1VSlC"
      },
      "outputs": [],
      "source": [
        "# Splitting the dataset into training and testing datasets\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    faces, ids, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Splitting the training dataset into training and validation datasets\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train, y_train, test_size=0.1, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "--- \n",
        "\n",
        "## <span style=\"color:#0b486b\">Modelling time!</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BaseModel(\n",
        "    name=\"Basic Model\",\n",
        "    input_width=RESIZED_SHAPES[0][0],\n",
        "    input_height=RESIZED_SHAPES[0][1],\n",
        "    depth=DEPTH,\n",
        "    num_classes=num_classes,\n",
        "    activation_func=RELU,\n",
        "    optimiser=ADAM_OPT,\n",
        ")\n",
        "\n",
        "model.build_cnn()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPEZMHd0WJWM"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lPSgS0XWL3n"
      },
      "outputs": [],
      "source": [
        "evaluation_results = model.compute_accuracy(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">ResNet Upgrade!</span>\n",
        "\n",
        "Now, we upgrade the model by implementing **ResNet blocks**, **Dropout layers** and **Batch Normalisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_model = ResNetModel(\n",
        "    name=\"ResNet Model\",\n",
        "    input_width=RESIZED_SHAPES[0][0],\n",
        "    input_height=RESIZED_SHAPES[0][1],\n",
        "    depth=DEPTH,\n",
        "    num_classes=num_classes,\n",
        "    activation_func=RELU,\n",
        "    optimiser=ADAM_OPT,\n",
        ")\n",
        "\n",
        "resnet_model.build_cnn()\n",
        "resnet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resnet_model.fit(x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_results = resnet_model.compute_accuracy(x_test=x_test, y_test=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <span style=\"color:#0b486b\">SkipConnection Upgrade!</span>\n",
        "\n",
        "Now, we upgrade the model by implementing **ResNet blocks**, **Dropout layers** and **Batch Normalisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 96, 96, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 96, 96, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 96, 96, 32)        0         \n",
            "                                                                 \n",
            " res_net_block (ResNetBlock)  (None, 48, 48, 32)       18752     \n",
            "                                                                 \n",
            " res_net_block_1 (ResNetBloc  (None, 24, 24, 64)       58048     \n",
            " k)                                                              \n",
            "                                                                 \n",
            " res_net_block_2 (ResNetBloc  (None, 12, 12, 128)      230784    \n",
            " k)                                                              \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18432)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 182)               3354806   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,663,414\n",
            "Trainable params: 3,662,454\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "skip_conn_model = SkipConnModel(\n",
        "    name=\"ResNet Model\",\n",
        "    input_width=RESIZED_SHAPES[0][0],\n",
        "    input_height=RESIZED_SHAPES[0][1],\n",
        "    depth=DEPTH,\n",
        "    num_classes=num_classes,\n",
        "    activation_func=RELU,\n",
        "    optimiser=ADAM_OPT,\n",
        ")\n",
        "\n",
        "skip_conn_model.build_cnn()\n",
        "skip_conn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "134/134 [==============================] - 250s 2s/step - loss: 5.2049 - accuracy: 0.0401\n",
            "Epoch 2/20\n",
            "134/134 [==============================] - 141s 1s/step - loss: 3.7066 - accuracy: 0.1978\n",
            "Epoch 3/20\n",
            "134/134 [==============================] - 126s 941ms/step - loss: 2.5601 - accuracy: 0.4121\n",
            "Epoch 4/20\n",
            "134/134 [==============================] - 149s 1s/step - loss: 1.7112 - accuracy: 0.5863\n",
            "Epoch 5/20\n",
            "134/134 [==============================] - 140s 1s/step - loss: 1.0905 - accuracy: 0.7489\n",
            "Epoch 6/20\n",
            "134/134 [==============================] - 134s 1s/step - loss: 0.7014 - accuracy: 0.8460\n",
            "Epoch 7/20\n",
            "134/134 [==============================] - 123s 915ms/step - loss: 0.4560 - accuracy: 0.9109\n",
            "Epoch 8/20\n",
            "134/134 [==============================] - 124s 925ms/step - loss: 0.2990 - accuracy: 0.9456\n",
            "Epoch 9/20\n",
            "134/134 [==============================] - 127s 947ms/step - loss: 0.2165 - accuracy: 0.9658\n",
            "Epoch 10/20\n",
            "134/134 [==============================] - 125s 932ms/step - loss: 0.1547 - accuracy: 0.9782\n",
            "Epoch 11/20\n",
            "134/134 [==============================] - 125s 936ms/step - loss: 0.1163 - accuracy: 0.9845\n",
            "Epoch 12/20\n",
            "134/134 [==============================] - 126s 940ms/step - loss: 0.0895 - accuracy: 0.9895\n",
            "Epoch 13/20\n",
            "134/134 [==============================] - 124s 927ms/step - loss: 0.0713 - accuracy: 0.9911\n",
            "Epoch 14/20\n",
            "134/134 [==============================] - 123s 916ms/step - loss: 0.0623 - accuracy: 0.9911\n",
            "Epoch 15/20\n",
            "134/134 [==============================] - 111s 827ms/step - loss: 0.0539 - accuracy: 0.9930\n",
            "Epoch 16/20\n",
            "134/134 [==============================] - 109s 813ms/step - loss: 0.0481 - accuracy: 0.9948\n",
            "Epoch 17/20\n",
            "134/134 [==============================] - 112s 835ms/step - loss: 0.0365 - accuracy: 0.9958\n",
            "Epoch 18/20\n",
            "134/134 [==============================] - 110s 824ms/step - loss: 0.0315 - accuracy: 0.9960\n",
            "Epoch 19/20\n",
            "134/134 [==============================] - 113s 840ms/step - loss: 0.0372 - accuracy: 0.9953\n",
            "Epoch 20/20\n",
            "134/134 [==============================] - 111s 824ms/step - loss: 0.0352 - accuracy: 0.9941\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1e18bc18610>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Stops early based on the validation loss\n",
        "# When running with GPU, could use higher patience level (cuz can afford to compute more hahah)\n",
        "# min_delta parameter is used to determine how much leeway to give for change of validation loss\n",
        "#       default is 0\n",
        "early_stopping = EarlyStopping(patience=2, monitor=\"val_loss\", mode=\"min\", min_delta=0)\n",
        "\n",
        "# Saves the best model based on validation loss\n",
        "val_loss_checkpoint = ModelCheckpoint(\n",
        "    os.path.join(\"./ckpts\", \"best_val\"),\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Saves the best model based on validation accuracy\n",
        "val_acc_checkpoint = ModelCheckpoint(\n",
        "    os.path.join(\"./ckpts\", \"best_acc\"),\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "skip_conn_model.fit(\n",
        "    x_train=x_train,\n",
        "    y_train=y_train,\n",
        "    x_val=x_val,\n",
        "    y_val=y_val,\n",
        "    callbacks=[early_stopping, val_loss_checkpoint, val_acc_checkpoint],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38/38 [==============================] - 15s 361ms/step - loss: 3.2098 - accuracy: 0.5008\n",
            "loss: 3.2097971439361572\n",
            "accuracy: 0.5008431673049927\n"
          ]
        }
      ],
      "source": [
        "evaluation_results = skip_conn_model.compute_accuracy(x_test=x_test, y_test=y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Version Overview**\n",
        "\n",
        "1.0.0\n",
        "- Copied file from jupyter notebook in MCS13 private Google Drive\n",
        "- Imported new BaseModel and constants files\n",
        "- Did basic training using BaseModel \n",
        "\n",
        "1.1.0\n",
        "- Imported ResNetModel\n",
        "- Included verbose during training in BaseModel class\n",
        "\n",
        "1.2.0\n",
        "- Introduced Skip Connection :D\n",
        "- Included callbacks parameter for .fit() function in BaseModel class"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

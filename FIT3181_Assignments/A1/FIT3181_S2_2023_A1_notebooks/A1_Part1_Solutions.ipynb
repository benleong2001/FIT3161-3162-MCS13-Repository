{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85398b33-3b67-4699-bf4f-99fb4dbef6b1",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">FIT3181 S2 2023 - ASSIGNMENT 1</span>\n",
    "\n",
    "<font size=\"+1\">Due: <span style=\"color:red\">[11:59pm, 08 September 2023]</span>  (Friday)</font>\n",
    "\n",
    "<font size=\"+1\">**Important note:**</font> This is an **individual** assignment. It contributes **25%** to your final mark. Read the assignment instruction carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6046830-b520-4f6f-a08e-bba5ede331fe",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  Student Information</span>\n",
    "***\n",
    "Surname: **Leong**  <br/>\n",
    "Firstname: **Benjamin Tjen Ho**    <br/>\n",
    "Student ID: **32809824**    <br/>\n",
    "Email: **bleo0009@student.monash.edu**    <br/>\n",
    "Your tutorial time: **Friday, 8am - 10am**    <br/>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce83d4f-48ef-426a-80dd-a03a3aa4ac06",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Instruction</span>\n",
    "\n",
    "This notebook has been prepared for your to complete Assignment 1. The theme of this assignment is about practical machine learning knowledge and skills in deep neural networks, including feedforward and convolutional neural networks. Some sections have been partially completed to help you get\n",
    "started. **The total marks for this notebook is 100**.\n",
    "\n",
    "* Before you start, read the entire notebook carefully once to understand what you need to do. <br/>\n",
    "\n",
    "* For each cell marked with **#YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL** or **INSERT YOUR CODES IN THIS CELL**, there will be places where you **must** supply your own codes when instructed. <br>\n",
    "\n",
    "This assignment contains **three** parts:\n",
    "\n",
    "* Part 1: Questions on theory and knowledge on machine learning and deep learning **[30 points]**\n",
    "* Part 2: Coding assessment on TensorFlow for Deep Neural Networks (DNN) **[30 points]**\n",
    "* Part 3: Coding assessment on TensorFlow for Convolution Neural Networks (CNN) **[40 points]**\n",
    "\n",
    "**Hint**: This assignment was essentially designed based on the lectures and tutorials sessions covered from Week 1 to Week 5. You are strongly recommended to go through these contents thoroughly which might help you to complete this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc31af24-7213-49f5-99b9-669c185735cc",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">What to submit</span>\n",
    "\n",
    "This assignment is to be completed individually and submitted to Moodle unit site. **By the due date, you are required to submit one  <span style=\"color:red; font-weight:bold\">single zip file, named xxx_assignment01_solution.zip</span> where `xxx` is your student ID, to the corresponding Assignment (Dropbox) in Moodle**. \n",
    "\n",
    "***For example, if your student ID is <span style=\"color:red; font-weight:bold\">12356</span>, then gather all of your assignment solution to folder, create a zip file named <span style=\"color:red; font-weight:bold\">123456_assignment01_solution.zip</span> and submit this file.***\n",
    "\n",
    "Within this zip folder, you **must** submit the following files:\n",
    "\n",
    "1. **A1_Part1_Solutions.ipynb, A1_Part2_Solutions.ipynb, A1_Part3_Solutions.ipynb**: these are your Python notebook solution source files.\n",
    "2. **A1_Part1_Solutions.html, A1_Part2_Solutions.html, A1_Part3_Solutions.html**: these are the output of your Python notebook solution *exported* in HTML format.\n",
    "3. Any **extra files or folder** needed to complete your assignment (e.g., images used in your answers).\n",
    "\n",
    "**You can run your codes on Google Colab. In this case, you need to capture the screenshots of your Google Colab model training and put in corresponding places in your Jupyter notebooks. You also need to store your trained models to folder <span style=\"color:red; font-weight:bold\">*./models*</span> with recognizable file names (e.g., Part3_Sec3_2_model.h5).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f83e10-3989-4235-a3ef-4cce62a57e39",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Theory and Knowledge Questions</span>\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[Total marks for this part: 30 points]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61388bef-2a30-4ecb-ae19-2713b7507936",
   "metadata": {},
   "source": [
    "The first part of this assignment is for you to demonstrate your knowledge in deep learning that you have acquired from the lectures and tutorials materials. Most of the contents in this assignment are drawn from **the lectures and labs from weeks 1 to 2**. Going through these materials before attempting this part is highly recommended.\n",
    "\n",
    "**Numpy** is possibly being used in the following questions. You need to import it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65effb9f-5798-43ae-82f6-71700b5e4728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from math import log, e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3947be-76c2-48ae-9d0d-d44a0565e6dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#0b486b\">Question 1.1: Activation Functions</span>\n",
    "Activation function plays an important role in modern Deep NNs. In this question, we will explore some of them to get deeper understanding of their characteristics and their advantages.\n",
    "\n",
    "**(a)** Given the Exponential Linear Unit activation function: $$\\text{ELU}\\left(x\\right)=\\begin{cases}\\alpha\\,(e^x-1) & \\text{if}\\,x<0\\ (\\alpha\\ \\text{is a hyper-parameter})\\\\x & \\text{otherwise}\\end{cases}$$\n",
    "\n",
    "State its output range, find its derivative (show your steps), and plot the activation fuction and its derivative.\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>\n",
    "\n",
    "**(b)** In literature, there are a wide range of activation functions that have been proposed. Do a research and select two (2) activation functions that were not discussed in the lecture (including `ReLU`, `sigmoid`, and `tanh`). For each of the selected activation function, do the following:\n",
    "\n",
    "- Find the research paper which propose the activation function.\n",
    "- Write a brief summary of the author's motivation which leads to the activation function (max 150 words).\n",
    "- Write a brief summary of advantages of the activation function (max 150 words).\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[6 points]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4feda-310c-4149-a82c-2c8f99260b5d",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Answers 1.1: Activation Functions</span>\n",
    "**(a)** The given $\\text{ELU}$ function is a step-wise function that has 2 parts, $\\text{ELU}(x)=x \\text{  and  } \\text{ELU}(x)=\\alpha\\,(e^x-1)$. We will consider the functions separately. For each of the parts, we will consider the minimum and maximum values of x in its domain values. \n",
    "\n",
    "### $\\text{ELU}(x)=x$  \n",
    "\n",
    "In this part, the domain of $x$ is $[0, \\infty]$. When $x$ is 0, $\\text{ELU}(x)$ will evaluate as:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\text{ELU}(x) &= x \\\\\n",
    "\\text{ELU}(0) &= 0 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "As $x$ approaches $\\infty$, $\\text{ELU}(x)$ will evaluate as:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\lim\\limits_{x \\, \\to \\, \\infty} \\text{ELU}(x) &= \\lim\\limits_{x \\, \\to \\, \\infty}x \\\\\n",
    "\\text{ELU}(\\infty) &= \\infty \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore, **in this part**, the output range would be $[0, \\infty]$\n",
    "\n",
    "### $\\text{ELU}(x) = \\alpha(e^x - 1)$\n",
    "\n",
    "We know that $\\alpha$ is a hyper-parameter. Hyper-parameters in the context of Neural Networks should not be negative. If $\\alpha$ was 0, then the $\\text{ELU}$ function would simply be a $\\text{ReLU}$ function. Therefore, we can safely assume that $\\alpha$ is some positive value. \n",
    "\n",
    "In this part, $x$ only has negative values. Therefore, the domain of $x$ is $[-\\infty, 0)$. When $x$ approaches 0, $\\text{ELU}(x)$ will evaluate to:\n",
    "$$\\begin{aligned}\n",
    "\\lim\\limits_{x \\, \\to \\,0}\\text{ELU}(x) &= \\lim\\limits_{x \\, \\to \\, 0}\\alpha\\,(e^x - 1) \\\\\n",
    "\\text{ELU}(0) &= \\alpha\\,(e^0 - 1) \\\\\n",
    "              &= \\alpha\\,(e^0 - 1) \\\\\n",
    "              &= \\alpha\\,(1 - 1) \\\\ \n",
    "              &= \\alpha\\,\\times0 \\\\ \n",
    "              &= 0 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Hence, we know that as $x$ approaches 0, the $\\text{ELU}$ function will also approach 0. \n",
    "<br>Now let's consider the following limit:\n",
    "$$\\lim\\limits_{x \\, \\to \\, -\\infty}\\alpha\\,(e^x-1)$$\n",
    "\n",
    "In the limit, as $x$ approaches $-\\infty$ in $e^x$, it will approach $e^{-\\infty}=\\frac{1}{e^\\infty}\\rightarrow0$. With that, we can easily evaluate $\\text{ELU}(x)$ as $x$ approaches $-\\infty$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\lim\\limits_{x \\, \\to \\, -\\infty} \\text{ELU}(x) &= \\lim\\limits_{x \\, \\to \\, -\\infty} \\alpha\\,(e^x-1) \\\\\n",
    "&=\\alpha\\,(0-1) \\\\\n",
    "&=\\alpha(-1)   \\\\\n",
    "&=-\\alpha \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore, **in this part**, the output range is $(-\\alpha, 0)$\n",
    "\n",
    "### Output Range\n",
    "With these values, we can see that the output range is **$(-\\alpha, \\infty]$**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99cc1c",
   "metadata": {},
   "source": [
    "## First Derivative\n",
    "If x < 0:\n",
    "$$\\begin{aligned}\n",
    "\\text{ELU}(x) &= \\alpha(e^x - 1) \\\\\n",
    "\\frac{d(\\text{ELU}(x))}{dx} &= \\frac{d}{dx}(\\alpha(e^x - 1)) \\\\\n",
    "&= \\alpha\\frac{d}{dx}(e^x - 1) \\\\\n",
    "&= \\alpha e^x\n",
    "\\end{aligned}$$\n",
    "\n",
    "Otherwise:\n",
    "$$\\begin{aligned}\n",
    "\\text{ELU}(x) &= x \\\\\n",
    "\\frac{d (\\text{ELU}(x))}{dx} &= \\frac{d}{dx}x \\\\\n",
    "&= 1 \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore, the first derivative of $\\text{ELU}(x)$ is:\n",
    "$$\n",
    "\\text{ELU}'(x)=\n",
    "\\begin{cases}\n",
    "\\alpha e^x & \\text{if}\\,x<0 \\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46746e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0JUlEQVR4nO3deXhU5cH+8TsJZNiSsIY1IAJCFUUQoagoKEIpWujlVvVFpGqVX6QiKBD2LSSAxQUQKSq4FqtWrWixUQT0FQVZFBBQFAyyI5CBAElIzu+P502GAIGEzJlnlu/nus41z0yGnHtEyM05zzlPlOM4jgAAACyIth0AAABELooIAACwhiICAACsoYgAAABrKCIAAMAaiggAALCGIgIAAKyhiAAAAGsq2A5wNgUFBdq5c6fi4uIUFRVlOw4AACgFx3F0+PBhNWjQQNHRZz/mEdRFZOfOnUpKSrIdAwAAnIft27erUaNGZ31PUBeRuLg4SeaDxMfHW04DAABKw+v1Kikpqejn+NkEdREpPB0THx9PEQEAIMSUZloFk1UBAIA1FBEAAGANRQQAAFhDEQEAANZQRAAAgDUUEQAAYA1FBAAAWEMRAQAA1lBEAACANa4WkXHjxikqKqrY1qpVKzd3CQAAQojrt3i/5JJL9PHHH/t2WCGo7yoPAAACyPVWUKFCBdWrV8/t3QAAgBDk+hyRH374QQ0aNNCFF16ou+++W5mZmSW+NycnR16vt9gGAAD879AhqXt36aSTFla4WkQ6duyo+fPna9GiRZo9e7a2bt2qzp076/Dhw2d8f1pamhISEoq2pKQkN+MBABCxpk6VMjKkRx6R8vPt5YhyHMcJ1M4OHTqkJk2aaPr06brvvvtO+3pOTo5ycnKKnnu9XiUlJSkrK0vx8fGBigkAQFjbuVNq3lw6dkx6912pd2//fn+v16uEhIRS/fwO6MzR6tWr66KLLtKWLVvO+HWPxyOPxxPISAAARJwJE0wJueoq6Q9/sJsloPcROXLkiH788UfVr18/kLsFAAD/5/vvpeefN+P0dCkqym4eV4vIY489pqVLl2rbtm364osv9Mc//lExMTG688473dwtAAAowciRZk5Ir15S586207h8auaXX37RnXfeqV9//VV16tTRNddcoy+//FJ16tRxc7cAAOAMVqyQ3nrLHAVJS7OdxnC1iCxYsMDNbw8AAErJcaRhw8z4nnukSy+1m6cQa80AABABPvpIWrJEio2Vxo+3ncaHIgIAQJgrKJBSUsz44YelJk3s5jkZRQQAgDC3YIG0dq0UHy+NGGE7TXEUEQAAwlhurjRqlBkPHSrVqmU3z6koIgAAhLE5c6StW6V69aRBg2ynOR1FBACAMHX4sDRxohmPGSNVrWo3z5lQRAAACFN/+5u0b5/UooV0//2205wZRQQAgDC0Z48pIpKUmipVrGg3T0koIgAAhKFJk6QjR6T27aVbb7WdpmQUEQAAwsxPP5lJqpI0ZYr9he3OhiICAECYGTVKysuTuneXrr/edpqzo4gAABBG1qyR/vEPM05Pt5ulNCgiAACEkcJbud95p9S2rd0spUERAQAgTCxebBa3q1DBd/+QYEcRAQAgDDiONHy4GT/0kNSsmd08pUURAQAgDLz9trRypbl7auHaMqGAIgIAQIjLy/OtqjtkiFS3rt08ZUERAQAgxL34ovTDD1KdOtJjj9lOUzYUEQAAQlh2tjRunBmPHi3FxVmNU2YUEQAAQtjTT0u7d0tNm0oPPmg7TdlRRAAACFG//mpu4S6ZtWViY+3mOR8UEQAAQtTkyZLXK11+ufSnP9lOc34oIgAAhKCff5ZmzjTjtDQpOkR/oodobAAAItvYsVJurtS1q9Sjh+00548iAgBAiFm3Tnr5ZTOeMkWKirKbpzwoIgAAhJgRI8wt3W+9VbrySttpyociAgBACPnsM2nhQikmRkpNtZ2m/CgiAACECMeRhg0z4/vvly66yG4ef6CIAAAQIv79b2n5cqlyZWnMGNtp/IMiAgBACMjP9y1sN2iQ1KCB1Th+QxEBACAEvPyy9N13Uo0a0tChttP4D0UEAIAgd+yY71TMyJFS9epW4/gVRQQAgCA3a5b0yy9SUpKUnGw7jX9RRAAACGKHDpk1ZSRpwgSpUiWrcfyOIgIAQBCbMkU6eFC65BKpb1/bafyPIgIAQJDasUN66ikzTkszNzELNxQRAACC1Pjx0vHj0tVXSzfdZDuNOygiAAAEoU2bpBdfNONQX9jubCgiAAAEoZEjzU3M/vAHc0QkXFFEAAAIMl9+Kf3rX1J0tO+KmXBFEQEAIIg4jjR8uBn362eulglnFBEAAILIokXS0qWSx2Mmq4Y7iggAAEGioMB3NGTgQHMn1XBHEQEAIEi8/rr07bdSQoKUkmI7TWBQRAAACAI5OdKoUWY8fLhUs6bdPIESsCKSnp6uqKgoDRo0KFC7BAAgZDz3nPTzz1L9+tJf/2o7TeAEpIisXLlSc+bM0WWXXRaI3QEAEFK8XmnSJDMeP16qUsVunkByvYgcOXJEd999t+bOnasaNWq4vTsAAELOE09I+/dLLVtK/fvbThNYrheR5ORk9erVS926dTvne3NycuT1eottAACEs927penTzXjyZKlCBbt5As3Vj7tgwQKtXr1aK1euLNX709LSND4SLpoGAOD/TJwoZWdLHTtKf/yj7TSB59oRke3bt+uRRx7Ra6+9pkqVKpXq16SkpCgrK6to2759u1vxAACwbssW6e9/N+P09PBd2O5sXDsismrVKu3du1ft2rUrei0/P1/Lli3TzJkzlZOTo5iYmGK/xuPxyOPxuBUJAICgMmqUdOKE1LOn1KWL7TR2uFZEbrjhBq1bt67Ya/3791erVq00bNiw00oIAACRZPVq6Y03zFGQtDTbaexxrYjExcWpdevWxV6rWrWqatWqddrrAABEmsJbud91l9Smjd0sNnFnVQAAAuzjj6WMDKliRTNZNZIF9CKhJUuWBHJ3AAAEnZMXthswQGra1G4e2zgiAgBAAL35prRqlRQX51tbJpJRRAAACJC8PGnkSDN+7DGpTh27eYIBRQQAgACZO1f68UcpMVEaPNh2muBAEQEAIACOHJEmTDDjMWOkatXs5gkWFBEAAALgySelPXukZs2kBx6wnSZ4UEQAAHDZvn3StGlmPGmSFBtrN08woYgAAOCyyZOlw4eltm2l22+3nSa4UEQAAHDRtm3Ss8+acXq6FM1P3mL4zwEAgIvGjJFyc6Xrr5duvNF2muBDEQEAwCXffiu9+qoZT5liFrhDcRQRAABckpIiOY6ZF9K+ve00wYkiAgCAC5YulT78UIqJMVfK4MwoIgAA+Jnj+Ba2e+ABqUULu3mCGUUEAAA/e/dd6csvpSpVzGRVlIwiAgCAH504IY0YYcaPPirVr283T7CjiAAA4Efz50ubNkm1akmPP247TfCjiAAA4CdHj0rjxpnxyJFSQoLVOCGBIgIAgJ/MmCHt2CE1biwNGGA7TWigiAAA4AcHDphbuEvSxIlSpUp284QKiggAAH6Qni4dOiRdeql0992204QOiggAAOX0yy/mtIwkpaWZm5ihdCgiAACU07hx0vHjUufO0u9/bztNaKGIAABQDt99J82bZ8YsbFd2FBEAAMph5EipoEDq00fq1Ml2mtBDEQEA4DwtX25u5x4dLU2ebDtNaKKIAABwHhxHGjbMjPv3l37zG7t5QhVFBACA8/DBB9Jnn5n7hRTeTRVlRxEBAKCM8vOllBQz/utfpUaN7OYJZRQRAADK6LXXpPXrperVpeHDbacJbRQRAADKICdHGjPGjIcPl2rUsJsn1FFEAAAog2eflX7+WWrY0JyWQflQRAAAKKWsLCk11YzHjZMqV7YaJyxQRAAAKKVp06Rff5VatZLuvdd2mvBAEQEAoBR27ZKmTzfjtDSpQgW7ecIFRQQAgFKYMEE6dkz67W+l3r1tpwkfFBEAAM7h+++luXPNmIXt/IsiAgDAOYwaZW5i1quXdO21ttOEF4oIAABn8fXX0ptvmqMgaWm204QfiggAACU4eWG7vn2lSy+1myccUUQAAChBRoa0eLEUG2smq8L/KCIAAJxBQYFvHZnkZKlJE7t5whVFBACAM3jjDWnNGik+Xhoxwnaa8EURAQDgFLm55koZSXr8cal2bbt5whlFBACAU/z979JPP0l160qPPmo7TXijiAAAcJLDh6WJE8147FipalW7ecKdq0Vk9uzZuuyyyxQfH6/4+Hh16tRJ//nPf9zcJQAA5TJ9urR3r9S8uXT//bbThD9Xi0ijRo2Unp6uVatW6euvv9b111+v3r17a8OGDW7uFgCA87J3r/TEE2acmipVrGg3TySIchzHCeQOa9asqWnTpum+++4753u9Xq8SEhKUlZWl+Pj4AKQDAESyv/5VmjFDat9e+uorKZoJDOelLD+/A7aIcX5+vt58801lZ2erU6dOZ3xPTk6OcnJyip57vd5AxQMARLiffpKee86M09MpIYHi+n/mdevWqVq1avJ4PHrooYf0zjvv6OKLLz7je9PS0pSQkFC0JSUluR0PAABJ0pgxUl6edOON0g032E4TOVw/NZObm6vMzExlZWXprbfe0vPPP6+lS5eesYyc6YhIUlISp2YAAK5au1Zq186sLbNqlRnj/JXl1EzA54h069ZNzZo105w5c875XuaIAAACoWdPadEi6U9/kv7xD9tpQl9Zfn4H/AxYQUFBsaMeAADYtGSJKSEVKkiTJtlOE3lcnayakpKinj17qnHjxjp8+LBef/11LVmyRB999JGbuwUAoFQcRxo61IwffFBq1sxunkjkahHZu3ev7rnnHu3atUsJCQm67LLL9NFHH+nGG290c7cAAJTK229LK1eau6eOHm07TWRytYi88MILbn57AADOW16eNHKkGQ8ZYtaVQeBxlTQAICK9+KL0/fdmZd0hQ2yniVwUEQBAxDl6VBo/3oxHj5a4MNMeiggAIOI8/bS0a5d0wQVmkirsoYgAACLKr79KU6aY8cSJksdjN0+ko4gAACJKWpqUlSW1aSPddZftNKCIAAAiRmamNHOmGbOwXXDgtwAAEDHGjpVycqQuXaQePWyngUQRAQBEiPXrpZdfNuMpU6SoKLt5YFBEAAARYcQIqaBAuuUWqUMH22lQiCICAAh7n38uvf++FBMjpabaToOTUUQAAGHNcaRhw8z4z3+WWra0mwfFUUQAAGHt/felL76QKlUyk1URXCgiAICwlZ8vpaSY8SOPSA0b2s2D01FEAABh6+WXpe++k2rUkIYPt50GZ0IRAQCEpePHfadiUlKk6tWtxkEJKCIAgLA0a5a0fbvUqJE0cKDtNCgJRQQAEHYOHfJdpjthgpmoiuBEEQEAhJ2pU6WDB6WLL5buucd2GpwNRQQAEFZ27pSeesqM09LMTcwQvCgiAICwMn68dOyYdPXV0s03206Dc6GIAADCxubN0gsvmHFaGgvbhQKKCAAgbIwcaW5idvPNUufOttOgNCgiAICw8NVX0ttvm6MgkyfbToPSoogAAEKe4/junHrPPVLr1nbzoPQoIgCAkPfRR9KSJZLHY+4bgtBBEQEAhLSCAt/Cdg8/LDVubDcPyoYiAgAIaQsWSGvXSvHxvkKC0EERAQCErNxcadQoMx42TKpVy24elB1FBAAQsubMkbZulerXlx55xHYanA+KCAAgJHm9vompY8dKVavazYPzQxEBAISkv/1N2r9fuugi6c9/tp0G54siAgAIOXv2mCIiSampUsWKdvPg/FFEAAAhZ9IkKTtb6tBBuuUW22lQHhQRAEBI+eknM0lVktLTWdgu1FFEAAAhZdQoKS9P6tFD6trVdhqUF0UEABAy1qyR/vEPM05Pt5sF/kERAQCEjMKF7e66S7r8cqtR4CcUEQBASPjkE+m//zVXyEycaDsN/IUiAgAIeo7jOxry0EPShRfazQP/oYgAAILeW29JX38tVavmW1sG4YEiAgAIanl50ogRZvzYY1Jiot088C+KCAAgqL3wgrRliykggwfbTgN/o4gAAIJWdrY0frwZjx4txcXZzQP/o4gAAILWU09Ju3ebyal/+YvtNHADRQQAEJT275emTjXjSZOk2Fi7eeAOV4tIWlqarrzySsXFxSkxMVF9+vTR5s2b3dwlACBMTJ4seb1S27bSHXfYTgO3uFpEli5dquTkZH355ZfKyMhQXl6eunfvruzsbDd3CwAIcT//LM2aZcZpaVI0x+/DVgU3v/miRYuKPZ8/f74SExO1atUqXXvttW7uGgAQwsaMkXJzpeuvl7p3t50GbnK1iJwqKytLklSzZs0zfj0nJ0c5OTlFz71eb0ByAQCCx7p10iuvmHF6uhQVZTcP3BWwg10FBQUaNGiQrr76arVu3fqM70lLS1NCQkLRlpSUFKh4AIAgkZJibul+223SlVfaTgO3RTmO4wRiRwMGDNB//vMfff7552rUqNEZ33OmIyJJSUnKyspSfHx8IGICACz67DPp2mulmBhp40apRQvbiXA+vF6vEhISSvXzOyCnZh5++GEtXLhQy5YtK7GESJLH45HH4wlEJABAkHEcadgwM37gAUpIpHC1iDiOo4EDB+qdd97RkiVL1LRpUzd3BwAIYe+9Jy1fLlWpYiarIjK4WkSSk5P1+uuv67333lNcXJx2794tSUpISFDlypXd3DUAIIScOOFb2G7QIKl+fatxEECuzhGJKmGq87x583Tvvfee89eX5RwTACB0vfCCdP/9Us2a0k8/SQkJthOhPIJmjkiA5sECAELYsWPS2LFmPHIkJSTScK86AIBVM2ZIO3ZIjRtL/+//2U6DQKOIAACsOXjQ3MJdkiZMkCpVspsHgUcRAQBYM2WKdOiQ1Lq19D//YzsNbKCIAACs2LFDevppM05PNzcxQ+ShiAAArBg3Tjp+XOrcWfr9722ngS0UEQBAwG3aJL34ohlPmcLCdpGMIgIACLgRI6SCAql3b6lTJ9tpYBNFBAAQUMuXS++8I0VHS5Mn204D2ygiAICAcRxp+HAzvvde6eKLrcZBEKCIAAAC5sMPpWXLJI/HTFYFKCIAgIDIz5dSUsz4r3+VkpLs5kFwoIgAAALi9deldeuk6tV9p2cAiggAwHU5OdLo0WY8fLhZZReQKCIAgACYPVv6+WepQQNp4EDbaRBMKCIAAFd5vVJqqhmPGydVqWI1DoIMRQQA4Kpp06T9+6WWLaX+/W2nQbChiAAAXLNrlzR9uhlPnixVqGA3D4IPRQQA4JqJE6WjR6WOHaU//tF2GgQjiggAwBU//CDNnWvGLGyHklBEAACuGDVKOnFC6tlTuu4622kQrCgiAAC/W7VK+uc/zVGQtDTbaRDMKCIAAL8rvHPqXXdJbdrYzYLgRhEBAPhVRob08cdSxYpmsipwNhQRAIDfFBT4joYMGCA1bWo3D4IfRQQA4Df//Ke0erUUF2cmqwLnQhEBAPhFbq6vfDz+uFSnjt08CA0UEQCAX8ydK/34o1S3rvToo7bTIFRQRAAA5XbkiDRhghmPGSNVq2Y3D0IHRQQAUG5PPint3Ss1ayY98IDtNAglFBEAQLns2ydNnWrGkyaZy3aB0qKIAADKJTXVnJpp1066/XbbaRBqKCIAgPO2bZs0e7YZp6dL0fxUQRnxvwwA4LyNHm0u273hBunGG22nQSiiiAAAzss330ivvWbGU6bYzYLQRREBAJyXlBTJcaQ77pCuuMJ2GoQqiggAoMyWLpX+8x+pQgVzpQxwvigiAIAycRxp2DAzfuABqXlzu3kQ2igiAIAyefdd6auvpCpVzF1UgfKgiAAASu3ECTM3RJIGD5bq1bObB6GPIgIAKLV586TNm6VatcwKu0B5UUQAAKVy9Kg0bpwZjxolxcdbjYMwQREBAJTKM89IO3dKTZpIAwbYToNwQREBAJzTgQPmFu6SNHGi5PHYzYPwQREBAJxTWpqUlSVdeql011220yCcUEQAAGe1fbs0Y4YZp6dLMTF28yC8uFpEli1bpptvvlkNGjRQVFSU3n33XTd3BwBwwbhxUk6OdO21Us+ettMg3LhaRLKzs9WmTRvNmjXLzd0AAFzy3XfS/PlmnJ4uRUVZjYMwVMHNb96zZ0/1pD4DwPlzHHPdrCXjh0qVCqQ/3Cx1ukxStrUocFOVKtZapqtFpKxycnKUk5NT9Nzr9VpMAwBB4OhRqVo1a7t/o3DwviR7MeC2I0ekqlWt7DqoJqumpaUpISGhaEtKSrIdCQDsOnHCdgLAVUF1RCQlJUWDBw8ueu71eikjACLbP/5hHmvUkL791jwGwIcfSrfdLlXySOvWSQ0aBGS3sKVKFWu7Dqoi4vF45OEuOQBgeL2+5W3HjZMaNQrIbvPzpWETpKOSBg6SGrQIyG4RoYLq1AwA4CSTJ0v79kkXXRTQe6q/+qq0fr05+DJsWMB2iwjl6hGRI0eOaMuWLUXPt27dqrVr16pmzZpq3Lixm7sGgNC2dav05JNm/MQTUsWKAdnt8ePS6NFmnJISsDNBiGCuFpGvv/5aXbt2LXpeOP+jX79+ml94YToA4HSPPy7l5ko33CDddFPAdvvss+ZOqo0aSQ8/HLDdIoK5WkS6dOkix3Hc3AUAhJ+FC6W33zb3Up8+PWD3d8jKklJTzXjcOKly5YDsFhGOOSIAEEyys6XkZDMePFi67LKA7XrqVLPK7m9+I/XrF7DdIsJRRAAgmIwbJ2VmSk2aSGPHBmy3u3b5pqRMnixVCKprKhHOKCIAECy++cbXBmbODOidLsePl44dk666SurdO2C7BSgiABAU8vKk++4zN/G49daATlD9/nvp+efNmIXtEGgUEQAIBpMmSatWmetln346oLseOdL0n5tukjp3DuiuAYoIAFi3YoXvcpXZswN6P/WVK6W33jJHQSZPDthugSIUEQCw6ehRqW9fc0jizjulO+4I2K4dRxo+3Iz79pUuvTRguwaKUEQAwKbHHjOTNBo2lGbNCuiu//tfafFiKTZWmjAhoLsGilBEAMCWBQvMqRhJmjcvoPdTLyjwHQ1JTjZXCwM2UEQAwIZNm6T77zfjkSOlG28M6O4XLJDWrpXi46URIwK6a6AYiggABFp2trlENztb6trV3MQjgHJzpVGjzHjoUKl27YDuHiiGIgIAgeQ40l/+Im3YINWvL73+ullTJoDmzDGL+9arJw0aFNBdA6ehiABAIKWm+srHggWmDQTQ4cPSxIlmPG5cQG/eCpwRRQQAAuWf/5RGjzbjWbOka68NeIS//U3at09q0UL6858DvnvgNBQRAAiEFSt8S9o++qj04IMBj7B3rykikjkwU7FiwCMAp6GIAIDbNm82908/ftw8TptmJcakSdKRI1L79mauLBAMKCIA4KbMTHNp7r59Urt2VianStJPP0nPPWfGLGyHYEIRAQC37N1rSsj27VKrVtKiRVJcnJUoo0ebBX67d5duuMFKBOCMKCIA4IbCEvL99+a2pRkZUp06VqKsWWMOxEjmaAgQTCrYDgAAYWfnTqlbN2njRnN5bkaG1KiRtTgpKebxzjultm2txQDOiCMiAOBPmZnSddeZEtKokbRsmblW1pLFi6WPPjJXyEyaZC0GUCKKCAD4y4YNUufO0pYtUtOm1kuI4/gWtnvwQenCC61FAUpEEQEAf1i8WLr6anNEpGVLaelSU0YsevttaeVKc/fUwrVlgGBDEQGA8nrlFel3v5OysqRrrpG++EJKSrIaKS/PLOorSUOGSHXrWo0DlIgiAgDnKy/PrBp3zz1mfMcdZmJqzZq2k+nFF80FO7VrmyICBCuumgGA87Fnj3T77WYeiGQOP0yYIEXb//dddrZZ0E4y9w+Jj7caBzgriggAlFVGhlk3Ztcuc4Oyl1+W+vSxnarI009Lu3ebKSoWlrQBysR+dQeAUJGTY85zdO9uSsjFF5vZoEFUQn79VZoyxYwnTpQ8Hrt5gHOhiABAaaxaJXXoIE2fbp4PGGBKSMuWdnOdYvJkyeuV2rQxNzADgh1FBADO5uhRaehQU0K+/dbM/nzvPenZZ6UqVWynK+bnn6WZM804PT0opqsA58QcEQA4E8eR3n9fevRRs3StJP3pT2YCRmKi3WwlGDtWys2VunSRevSwnQYoHYoIAJxq40ZzWe5//2ueN2wozZ4t3Xyz1Vhns369mTMrmTkiUVF28wClxYE7ACi0fbt0//3SpZeaEhIba+6RvnFjUJcQSRoxwhzEueUWcxYJCBUcEQGAnTuladPMUY+cHPNa797SE09IzZvbzVYKn39uziLFxEipqbbTAGVDEQEQuX78UZo6VZo/30yukMzKuZMnS1ddZTVaaTmONGyYGd93X9BdxAOcE0UEQGRxHOnTT6VnnpH+/W/zXDKr5o4aJd14Y0hNsHj/fbO0TeXKZrIqEGooIgAiw6+/Sq++Ks2dK23Y4Hu9Z08pJcUUkRBz4oSJLpm5tQ0aWI0DnBeKCIDwlZtrJp2++qr0zju+0y9VqphbtA8cKP3mN3YzlsPLL0vffWfW2Bs61HYa4PxQRACEl7w8c+rl7belt96SDhzwfe3yy81VMXffLVWvbiuhXxw75jsVM2JEyH8cRDCKCIDQd+CAOfLxwQfSwoXSoUO+r9WrZ25E1rev1K6dtYj+NmuW9MsvUlKSlJxsOw1w/igiAEJPTo60fLm0eLH08cfSV19JBQW+r9etaxaiu/VWqWtXc11rGDl0yFzYI0kTJkiVKlmNA5QLRQRA8DtwQFqxwtww4/PPTfE4frz4e1q3ln7/e3PjsU6dwq58nGzKFOngQbP4b9++ttMA5UMRARBcDh6U1q6V1qyRVq82BeSHH05/X7160vXXmyMePXqYcxQRYMcOs9yNJKWlhXXfQoSgiACw4+BBadMms23cKK1bZxZM+eWXM7+/RQtzk7HOnaWrrzZ37gqh+334y/jxZqLq1VcH/V3ngVKhiABwR26u+ef7zz9LW7f6ti1bzLZ/f8m/9oILpLZtzdahg3TlleYa1Qi3aZP04otmzMJ2CBcBKSKzZs3StGnTtHv3brVp00YzZsxQB1ZlAkLTsWPS3r2+bdcuafdus17Lzp2mfPzyi7Rnj++upSVp1Ehq1coc3Wjd2iw2d8klXItagpEjpfx86Q9/MEdEgHDgehF54403NHjwYD333HPq2LGjnnrqKfXo0UObN29WYmKi27sHcKoTJ6QjRySvVzp82DxmZfkeDx40l2UcPGgmiR48aO5Kun+/2Y4eLf2+PB6pcWNzhKNpU7M1b25OszRrJlWr5tanDDtffSX9619SdLTvihkgHEQ5zrn+yVI+HTt21JVXXqmZM2dKkgoKCpSUlKSBAwdq+PDhZ/21Xq9XCQkJysrKUnx8vJsxAfc4jvln7IkTvi0vz/d46pab63vMzTWXqhY+nrwdP158O3bMtx09ah6zs4tvR474Vpctj9hYKTHRbPXrm4mj9epJDRv6tsaNpdq1OX/gB45j5uQuXSr17+87PQMEq7L8/Hb1iEhubq5WrVqllMLFECRFR0erW7duWr58+Wnvz8nJUc5Jf0l6vV53gm3aZJb7DqTy9L3S/NqS3lOe1/09dpxzv17Sa6duJb1euBUUnHl88vOTH08dn7rl5xcfFz4vHJe0nThR/P4WwSQ2VoqLk+LjpYQE32P16r6tVi0zN6NmTVMqCrf4eApGAE2ebEqIx2MmqwLhxNUisn//fuXn56tu3brFXq9bt642bdp02vvT0tI0PhB/yjIzzcqbgE3R0VLFimfePB5TFE4ex8aaO1d5PGarVMn3vHLl4luVKr7HqlV9W1ycOR1SrZr5fgh6eXnSpElm3Lp1xFyljAgSVFfNpKSkaPDgwUXPvV6vktz4U9e0qVmcoazc+hdgWb9vad5f0ntK83pJ47K8PyqqbO85dXyur5dli44uPi58XtI4JqbkceHzkx9PHVeoUPwxJsYUisLnFSv6fg1wDvPmmTNvFStKb7xhOw3gf64Wkdq1aysmJkZ79uwp9vqePXtUr169097v8Xjk8XjcjGS0aCGlprq/HwAoB69XGj3ajJ94wszvBcKNq/8ki42N1RVXXKFPPvmk6LWCggJ98skn6tSpk5u7BoCQl5ZmrpC+6CJpwADbaQB3uH5qZvDgwerXr5/at2+vDh066KmnnlJ2drb69+/v9q4BIGRt2yY9+aQZP/GEOTUDhCPXi8gdd9yhffv2acyYMdq9e7cuv/xyLVq06LQJrAAAn8ceM1da33CDdNNNttMA7nH9PiLlwX1EAESihQvNOjIxMWbdv8sus50IKJuy/Pxm2j4ABJHsbCk52YwHD6aEIPxRRAAgiIwbZ2511KSJNHas7TSA+ygiABAkvvnGN0F15kxzDzog3FFEACAI5OVJ991nVga49VYmqCJyUEQAIAhMmiStWiXVqCE9/bTtNEDgUEQAwLIVK3w3e549W2rQwG4eIJAoIgBg0dGjUt++5pTMnXdKd9xhOxEQWBQRALDoscek77+XGjaUZs2ynQYIPIoIAFiyYIE5FSOZVXZr1LCbB7CBIgIAFmzaJN1/vxmPHCndeKPdPIAtFBEACLDsbHOJbna21LWrNH687USAPRQRAAggx5H+8hdpwwapfn3p9dfNmjJApKKIAEAApab6yseCBVK9erYTAXZRRAAgQP75T2n0aDOeNUu69lq7eYBgQBEBgABYsULq18+MH31UevBBu3mAYEERAQCXbd5s1o45ftw8TptmOxEQPCgiAOCizExzae6+fVK7dkxOBU5FEQEAl+zda0rI9u1Sq1bSokVSXJztVEBwoYgAgAsKS8j330tNmkgZGVKdOrZTAcGngu0AABBudu6UunWTNm40l+dmZEiNGtlOBQQnjogAgB9lZkrXXWdKSKNG0rJlUosWtlMBwYsiAgB+smGD1LmztGWL1LQpJQQoDYoIAPjB4sXS1VebIyItW0pLl5oyAuDsKCIAUE6vvCL97ndSVpZ0zTXSF19ISUm2UwGhgSICAOcpL08aNEi65x4zvuMOMzG1Zk3byYDQwVUzAHAe9uyRbr/dzAORpJEjpQkTpGj+eQeUCUUEAMooI8OsG7Nrl7lB2csvS3362E4FhCa6OwCUUk6ONGSI1L27KSEXXyytXEkJAcqDIgIApbBqldShgzR9unk+YIApIS1b2s0FhDqKCACcxdGj0tChpoR8+61Uu7b03nvSs89KVarYTgeEPuaIAMAZOI70/vvSo49KP/1kXvvTn6Snn5YSE+1mA8IJRQQATrFxo7ks97//Nc8bNpRmz5ZuvtlqLCAscWoGAP7P9u3S/fdLl15qSkhsrDR8uCkmlBDAHRwRARDxdu6Upk0zRz1ycsxrvXtLTzwhNW9uNxsQ7igiACLWjz9KU6dK8+dLubnmteuukyZPlq66ymo0IGJQRABEFMeRPv1UeuYZ6d//Ns8ls2ruqFHSjTdKUVF2MwKRhCICICL8+qv06qvS3LnShg2+13v2lFJSTBEBEHgUEQBhKzfXTDp99VXpnXd8p1+qVDG3aB84UPrNb+xmBCIdRQRAWMnLM6de3n5beust6cAB39cuv9xcFXP33VL16rYSAjgZRQRAyDtwwBz5+OADaeFC6dAh39fq1TM3IuvbV2rXzlpEACWgiAAIOTk50vLl0uLF0scfS199JRUU+L5et65ZiO7WW6WuXaWYGGtRAZwDRQRA0DtwQFqxQvr8c7N99ZV0/Hjx97RuLf3+9+bGY506UT6AUEERARBUDh6U1q6V1qyRVq82BeSHH05/X7160vXXmyMePXpISUkBjwrADygiAKw4eFDatMlsGzdK69ZJ69dLv/xy5vc3b25uMta5s3TNNVLLltzvAwgHFBEArsjNlXbskH7+Wdq61bdt2WK2/ftL/rUXXCC1bWu2Dh2k9u2lWrUCFh1AALlWRFJTU/XBBx9o7dq1io2N1aGTp7EDCFnHjkl79/q23bvNtnOnKR47dpijGnv2+O5aWpJGjaRWrczRjdatzWJzl1zCpbVAJHGtiOTm5uq2225Tp06d9MILL7i1GwBldOKEdOSI5PVKhw+bx6ws3+PBg+by14MHzSTRgwfNXUn37zfb0aOl35fHIzVubI5wNG1qtubNpRYtpGbNpGrV3PqUAEKFa0Vk/PjxkqT58+e7tQsgJDiOlJ9vCkDhlpfnezx1y831PebmmktVCx9P3o4fL74dO+bbjh41j9nZxbcjR3yry5ZHbKyUmGi2+vXNxNF69aSGDaUGDcxj48ZSnTrM4wBwdkE1RyQnJ0c5J/0t6fV6XdnPpk3Sc8/5np/r8LG/lXV/pXl/Se8pz+v+HjvOuV8v6bVTx+faCgrOPD75+cmPp45P3fLzi48LnxeOS9pOnCh+f4tgUrGiFB9vtoQE32P16r6tVi2pZk2z1a7t2+LjKRgA/COoikhaWlrRkRQ3ZWZKTz/t+m6As4qONmXgTJvHY446nDyOjTXjwq1SJd9WuXLxrUoV32PVqr6tWjXf5vHY/i8AAGUsIsOHD9eUKVPO+p6NGzeqVatW5xUmJSVFgwcPLnru9XqV5MLNAZo2lUaMKN/38Oe/Bsv6vUrz/pLeU5rXSxqX5f1RUWV7z6njc329LFt0dPFx4fOTX4+J8T0/1zg62jeOiTl9q1Ch+GNMjCkUhc8rVvT9egCIdGUqIkOGDNG999571vdceOGF5x3G4/HIE4B/prVoIaWmur4bAABwDmUqInXq1FGdOnXcygIAACKMa3NEMjMzdeDAAWVmZio/P19r166VJDVv3lzVuGYPAADIxSIyZswYvfTSS0XP27ZtK0n69NNP1aVLF7d2CwAAQkiU4wT64tXS83q9SkhIUFZWluLj423HAQAApVCWn9/M2wcAANZQRAAAgDUUEQAAYA1FBAAAWEMRAQAA1lBEAACANRQRAABgDUUEAABYQxEBAADWuHaLd38ovOmr1+u1nAQAAJRW4c/t0ty8PaiLyOHDhyVJSUlJlpMAAICyOnz4sBISEs76nqBea6agoEA7d+5UXFycoqKi/Pq9vV6vkpKStH379ohYx4bPG974vOGNzxv+wu0zO46jw4cPq0GDBoqOPvsskKA+IhIdHa1GjRq5uo/4+Piw+E0vLT5veOPzhjc+b/gLp898riMhhZisCgAArKGIAAAAayK2iHg8Ho0dO1Yej8d2lIDg84Y3Pm944/OGv0j8zIWCerIqAAAIbxF7RAQAANhHEQEAANZQRAAAgDUUEQAAYE1EFpHU1FRdddVVqlKliqpXr37G92RmZqpXr16qUqWKEhMT9fjjj+vEiROBDeqS77//Xr1791bt2rUVHx+va665Rp9++qntWK764IMP1LFjR1WuXFk1atRQnz59bEdyXU5Oji6//HJFRUVp7dq1tuO4Ztu2bbrvvvvUtGlTVa5cWc2aNdPYsWOVm5trO5rfzJo1SxdccIEqVaqkjh07asWKFbYjuSItLU1XXnml4uLilJiYqD59+mjz5s22YwVMenq6oqKiNGjQINtRAioii0hubq5uu+02DRgw4Ixfz8/PV69evZSbm6svvvhCL730kubPn68xY8YEOKk7brrpJp04cUKLFy/WqlWr1KZNG910003avXu37WiuePvtt9W3b1/1799f33zzjf73f/9Xd911l+1Yrhs6dKgaNGhgO4brNm3apIKCAs2ZM0cbNmzQk08+qeeee04jRoywHc0v3njjDQ0ePFhjx47V6tWr1aZNG/Xo0UN79+61Hc3vli5dquTkZH355ZfKyMhQXl6eunfvruzsbNvRXLdy5UrNmTNHl112me0ogedEsHnz5jkJCQmnvf7hhx860dHRzu7du4temz17thMfH+/k5OQEMKH/7du3z5HkLFu2rOg1r9frSHIyMjIsJnNHXl6e07BhQ+f555+3HSWgPvzwQ6dVq1bOhg0bHEnOmjVrbEcKqKlTpzpNmza1HcMvOnTo4CQnJxc9z8/Pdxo0aOCkpaVZTBUYe/fudSQ5S5cutR3FVYcPH3ZatGjhZGRkONddd53zyCOP2I4UUBF5RORcli9frksvvVR169Yteq1Hjx7yer3asGGDxWTlV6tWLbVs2VIvv/yysrOzdeLECc2ZM0eJiYm64oorbMfzu9WrV2vHjh2Kjo5W27ZtVb9+ffXs2VPr16+3Hc01e/bs0QMPPKBXXnlFVapUsR3HiqysLNWsWdN2jHLLzc3VqlWr1K1bt6LXoqOj1a1bNy1fvtxissDIysqSpLD4vTyb5ORk9erVq9jvcyShiJzB7t27i5UQSUXPQ/30RVRUlD7++GOtWbNGcXFxqlSpkqZPn65FixapRo0atuP53U8//SRJGjdunEaNGqWFCxeqRo0a6tKliw4cOGA5nf85jqN7771XDz30kNq3b287jhVbtmzRjBkz9OCDD9qOUm779+9Xfn7+Gf8+CvW/i86loKBAgwYN0tVXX63WrVvbjuOaBQsWaPXq1UpLS7MdxZqwKSLDhw9XVFTUWbdNmzbZjuma0n5+x3GUnJysxMREffbZZ1qxYoX69Omjm2++Wbt27bL9MUqttJ+3oKBAkjRy5EjdcsstuuKKKzRv3jxFRUXpzTfftPwpSq+0n3fGjBk6fPiwUlJSbEcut/P5M71jxw797ne/02233aYHHnjAUnL4Q3JystavX68FCxbYjuKa7du365FHHtFrr72mSpUq2Y5jTQXbAfxlyJAhuvfee8/6ngsvvLBU36tevXqnzUrfs2dP0deCUWk//+LFi7Vw4UIdPHiwaKnpZ599VhkZGXrppZc0fPjwAKQtv9J+3sJydfHFFxe97vF4dOGFFyozM9PNiH5Vlt/f5cuXn7ZeRfv27XX33XfrpZdecjGlf5X1z/TOnTvVtWtXXXXVVfr73//ucrrAqF27tmJiYor+/im0Z8+eoP27yB8efvhhLVy4UMuWLVOjRo1sx3HNqlWrtHfvXrVr167otfz8fC1btkwzZ85UTk6OYmJiLCYMjLApInXq1FGdOnX88r06deqk1NRU7d27V4mJiZKkjIwMxcfHF/uBFkxK+/mPHj0qyZxnPll0dHTR0YNQUNrPe8UVV8jj8Wjz5s265pprJEl5eXnatm2bmjRp4nZMvynt533mmWc0adKkouc7d+5Ujx499MYbb6hjx45uRvS7svyZ3rFjh7p27Vp0xOvU/79DVWxsrK644gp98sknRZecFxQU6JNPPtHDDz9sN5wLHMfRwIED9c4772jJkiVq2rSp7UiuuuGGG7Ru3bpir/Xv31+tWrXSsGHDIqKESGFURMoiMzNTBw4cUGZmpvLz84vusdC8eXNVq1ZN3bt318UXX6y+fftq6tSp2r17t0aNGqXk5OSQXxmxU6dOqlGjhvr166cxY8aocuXKmjt3rrZu3apevXrZjud38fHxeuihhzR27FglJSWpSZMmmjZtmiTptttus5zO/xo3blzsebVq1SRJzZo1C9t/We7YsUNdunRRkyZN9MQTT2jfvn1FXwuHowaDBw9Wv3791L59e3Xo0EFPPfWUsrOz1b9/f9vR/C45OVmvv/663nvvPcXFxRXNg0lISFDlypUtp/O/uLi40+a/VK1aVbVq1QrreTGnsXzVjhX9+vVzJJ22ffrpp0Xv2bZtm9OzZ0+ncuXKTu3atZ0hQ4Y4eXl59kL70cqVK53u3bs7NWvWdOLi4pzf/va3zocffmg7lmtyc3OdIUOGOImJiU5cXJzTrVs3Z/369bZjBcTWrVvD/vLdefPmnfHPczj99TZjxgyncePGTmxsrNOhQwfnyy+/tB3JFSX9Ps6bN892tICJxMt3oxzHcQJdfgAAAKQwumoGAACEHooIAACwhiICAACsoYgAAABrKCIAAMAaiggAALCGIgIAAKyhiAAAAGsoIgAAwBqKCAAAsIYiAgAArKGIAAAAa/4/Jd0dhmisVbkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "# Define a Function\n",
    "def elu1(x):\n",
    "    return alpha * (math.e ** x - 1)\n",
    "\n",
    "def elu2(x):\n",
    "    return x\n",
    "\n",
    "def deriv1(x):\n",
    "    return alpha * (math.e ** x)\n",
    "\n",
    "# Create x and y Ranges\n",
    "x1 = np.linspace(-10, 0, 100)\n",
    "x2 = np.linspace(0, 5, 100)\n",
    "y1 = elu1(x1)\n",
    "y2 = elu2(x2)\n",
    "d1 = deriv1(x1)\n",
    "d2 = [1 for _ in x2]\n",
    "\n",
    "# # Plot the Data\n",
    "plt.plot(x1, y1, color=\"b\")\n",
    "plt.plot(x2, y2, color=\"b\")\n",
    "plt.plot(x1, d1, color=\"r\")\n",
    "plt.plot(x2, d2, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb2d61",
   "metadata": {},
   "source": [
    "**Question 1.1(b)**\n",
    "**Swish**\n",
    "\n",
    "**ACTIVATION_FUNCTION_2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec190b-a0eb-4398-8fe4-5c846a826e1c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">Question 1.2: Feed-forward neural networks</span>\n",
    "Assume that we feed a data point $x$ with a ground-truth label $y=3$ _(with index starting from 1 as in the lecture)_ to the feed-forward neural network with the ReLU activation function at hidden layers as shown in the following figure:\n",
    "\n",
    "<img src=\"Figures/Q2_P1_S2_2023.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\bar h^k(x) &= W^k h^{k - 1} + b^k \\\\\n",
    "h^k(x) &= \\sigma(\\bar h^k(x)) \\\\\n",
    "\\sigma(x) &= ReLU(x) = max(0, x) \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d546f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1, -1, 0], shape=[3, 1])\n",
    "\n",
    "# Setting value at index 0 as None so the first matrix value is at index 1\n",
    "w_values = [\n",
    "    None,\n",
    "    tf.constant([1, -1, 1, 1, -1, -1, 2, -1, 2, -1, -2, 1], shape=[4, 3]),\n",
    "    tf.constant([1, -1, -1, 2, 1, -1, 1, -1, 1, 2, -1, 2], shape=[3, 4]),\n",
    "    tf.constant([1, -2, 1, 1, 2, -1, -1, 1, -1], shape=[3, 3])\n",
    "]\n",
    "\n",
    "b_values = [\n",
    "    None, \n",
    "    tf.constant([1, 0, 1, 0], shape=[4, 1]),\n",
    "    tf.constant([-1, 1, 0], shape=[3, 1]),\n",
    "    tf.constant([4, -2, 2], shape=[3, 1])\n",
    "]\n",
    "\n",
    "def ReLU(x): return max(0, x)\n",
    "\n",
    "def ReLU_matrix(mat):\n",
    "    shape = mat.shape\n",
    "    return tf.constant([[num if num == 0 else num.numpy() for num in [ReLU(mat[y][x]) for x in range(shape[1])]] for y in range(shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463f640",
   "metadata": {},
   "source": [
    "**(a)** What is the numerical value of the latent presentation $h^1(x)$\n",
    "$$\\begin{aligned}\n",
    "W^1 = \\begin{bmatrix} \n",
    "        1 & -1 & 1 \\\\\n",
    "        1 & -1 & -1 \\\\\n",
    "        2 & -1 & 2 \\\\\n",
    "        -1 & -2 & 1\n",
    "        \\end{bmatrix},\\ \\ h^0(x) &= x = \\begin{bmatrix} \n",
    "        1 \\\\ -1 \\\\ 0\n",
    "        \\end{bmatrix},\\ \\ b^1 = \\begin{bmatrix} \n",
    "        1 \\\\ 0 \\\\ 1 \\\\ 0\n",
    "        \\end{bmatrix} \n",
    "\\\\\n",
    "\\\\\n",
    "\\bar h^1(x) &= W^1 h^{1 - 1}(x) + b^1 \n",
    "            = W^1 h^0(x) + b^1 \\\\\n",
    "            &= W^1 x + b^1 \\\\\n",
    "            &= \\begin{bmatrix} 1 & -1 & 1 \\\\\n",
    "                                1 & -1 & -1 \\\\\n",
    "                                2 & -1 & 2 \\\\\n",
    "                                -1 & -2 & 1 \\end{bmatrix} \n",
    "                \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\\n",
    "            &= \\begin{bmatrix} 1(1) + (-1)(-1) + 1(0) \\\\\n",
    "                                1(1) + (-1)(-1) + (-1)(0) \\\\\n",
    "                                2(1) + (-1)(-1) + 2(0) \\\\\n",
    "                                -1(1) + (-2)(-1) + 1(0) \\end{bmatrix}\n",
    "                \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\\n",
    "            &= \\begin{bmatrix} 2 \\\\ 2 \\\\ 3 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \n",
    "            = \\begin{bmatrix} 2 + 1 \\\\ 2 + 0 \\\\ 3 + 1 \\\\ 1 + 0 \\end{bmatrix} \n",
    "            = \\begin{bmatrix} 3 \\\\ 2 \\\\ 4 \\\\ 1 \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "h^1(x) &= \\sigma(\\bar h^1(x)) \\\\\n",
    "        &= \\text{ReLU}\\left(\\begin{bmatrix} 3 \\\\ 2 \\\\ 4 \\\\ 1 \\end{bmatrix}\\right)\n",
    "        = \\begin{bmatrix} 3 \\\\ 2 \\\\ 4 \\\\ 1 \\end{bmatrix} \\\\\n",
    "\\end{aligned}$$\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a790ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "tf.Tensor(\n",
      "[[3]\n",
      " [2]\n",
      " [4]\n",
      " [1]], shape=(4, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "part_a_h_bar_1 = tf.add(tf.matmul(w_values[1], x), b_values[1])\n",
    "part_a_h_1 = ReLU_matrix(part_a_h_bar_1)\n",
    "\n",
    "print(part_a_h_bar_1.shape)\n",
    "print(part_a_h_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3251414e",
   "metadata": {},
   "source": [
    "\n",
    "**(b)** What is the numerical value of the latent presentation $h^2(x)$?\n",
    "\n",
    "$$\\begin{aligned}\n",
    "W^2 = \\begin{bmatrix} \n",
    "        1 & -1 & -1 & 2 \\\\\n",
    "        1 & -1 & 1 & -1 \\\\\n",
    "        1 & 2 & -1 & 2 \n",
    "        \\end{bmatrix},\\ \\ \n",
    "h^1(x) &= x = \\begin{bmatrix} \n",
    "        3 \\\\ 2 \\\\ 4 \\\\ 1\n",
    "        \\end{bmatrix}, \\ \\ \n",
    "b^2 = \\begin{bmatrix} \n",
    "        -1 \\\\ 1 \\\\ 0\n",
    "        \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "\\bar h^2(x) &= W^2 h^{2 - 1}(x) + b^2 \\\\\n",
    "        &= W^2 h^1(x) + b^2 \\\\\n",
    "        &= \\begin{bmatrix} 1 & -1 & -1 & 2 \\\\\n",
    "                        1 & -1 & 1 & -1 \\\\\n",
    "                        1 & 2 & -1 & 2 \\end{bmatrix} \n",
    "        \\begin{bmatrix} 3 \\\\ 2 \\\\ 4 \\\\ 1 \\end{bmatrix} \n",
    "        + \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\\n",
    "        &= \\begin{bmatrix}\n",
    "        1(3) + (-1)(2) + (-1)(4) + 2(1) \\\\\n",
    "        1(3) + (-1)(2) + 1(4) + (-1)(1) \\\\\n",
    "        1(3) + 2(2) + (-1)(4) + 2(1)\n",
    "        \\end{bmatrix}\n",
    "        + \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix} \\\\\n",
    "        &= \\begin{bmatrix} -1 \\\\ 4 \\\\ 5 \\end{bmatrix} + \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix} \n",
    "        = \\begin{bmatrix} -2 \\\\ 5 \\\\ 5 \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "h^2(x) &= \\sigma(\\bar h^2(x)) \\\\\n",
    "        &= \\text{ReLU}\\left(\\begin{bmatrix} -2 \\\\ 5 \\\\ 5 \\end{bmatrix}\\right)\n",
    "        = \\begin{bmatrix} 0 \\\\ 5 \\\\ 5 \\end{bmatrix} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe35d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_b_h_bar_2 = tf.add(tf.matmul(w_values[2], part_a_h_1), b_values[2])\n",
    "part_b_h_2 = ReLU_matrix(part_b_h_bar_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb2ecb",
   "metadata": {},
   "source": [
    "**(c)** What is the numerical value of the logit $h^3(x)$?\n",
    "$$\\begin{aligned}\n",
    "W^3 = \\begin{bmatrix} \n",
    "        1 & -2 & 1 \\\\\n",
    "        1 & 2 & -1 \\\\\n",
    "        -1 & 1 & -1 \\\\\n",
    "        \\end{bmatrix}, \\ \\ \n",
    "h^2(x) &= x = \\begin{bmatrix} \n",
    "        0 \\\\ 5 \\\\ 5 \n",
    "        \\end{bmatrix}, \\ \\ \n",
    "b^3 = \\begin{bmatrix} \n",
    "        4 \\\\ -2 \\\\ 2\n",
    "        \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "h^3(x) &= W^3 h^{3 - 1}(x) + b^3 \n",
    "        = W^3 h^2(x) + b^3 \\\\\n",
    "        &= \\begin{bmatrix} \n",
    "                        1 & -2 & 1 \\\\\n",
    "                        1 & 2 & -1 \\\\\n",
    "                        -1 & 1 & -1 \n",
    "                        \\end{bmatrix}\n",
    "        \\begin{bmatrix} 0 \\\\ 5 \\\\ 5 \\end{bmatrix} \n",
    "        + \\begin{bmatrix} 4 \\\\ -2 \\\\ 2 \\end{bmatrix} \\\\\n",
    "        &= \\begin{bmatrix} \n",
    "                        1(0) & -2(5) & 1(5) \\\\\n",
    "                        1(0) & 2(5) & -1(5) \\\\\n",
    "                        -1(0) & 1(5) & -1(5) \n",
    "                        \\end{bmatrix}\n",
    "        + \\begin{bmatrix} 4 \\\\ -2 \\\\ 2 \\end{bmatrix} \\\\\n",
    "        &= \\begin{bmatrix} -5 \\\\ 5 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 4 \\\\ -2 \\\\ 2 \\end{bmatrix}\n",
    "        = \\begin{bmatrix} -1 \\\\ 3 \\\\ 2 \\end{bmatrix} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a043cfb",
   "metadata": {},
   "source": [
    "**(d)** What is the corresonding prediction probabilities $p(x)$?\n",
    "\n",
    "$$p(x_i) = softmax(h^3(x_i)) = \\frac{e^{x_i}}{\\sum_{j = 1}^{n}e^{x_j}}$$\n",
    "\n",
    "$$h^3(x) = \\begin{bmatrix} -1 \\\\ 3 \\\\ 2 \\end{bmatrix}$$\n",
    "\n",
    "$\\begin{aligned}\n",
    "Let\\ denom &= \\sum\\limits_{j = 1}^{3}e^{x_j}\n",
    "        &= e^{x_1} + e^{x_2} + e^{x_3}\n",
    "        &= e^{-1} + e^3 + e^2 \n",
    "\\end{aligned}$\n",
    "\n",
    "Prediction probabilities:\n",
    "$$\\begin{aligned}\n",
    "p(x) &= \\begin{bmatrix}\n",
    "        p(x_1) \\\\\n",
    "        p(x_2) \\\\\n",
    "        p(x_3)\n",
    "        \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "p(x_1) &= softmax(h^3(x_1)) \\\\\n",
    "        &= \\frac{e^{x_1}}{denom}\n",
    "        &= \\frac{e^{-1}}{denom}\n",
    "        = \\frac{e^{-1}}{1 + e^4 + e^5}\n",
    "        = 0.01321289\n",
    "\\\\\n",
    "p(x_2) &= softmax(h^3(x_2)) \\\\\n",
    "        &= \\frac{e^{x_2}}{denom}\n",
    "        &= \\frac{e^3}{denom}\n",
    "        = \\frac{e^3}{1 + e^4 + e^5}\n",
    "        = 0.7213992\n",
    "\\\\\n",
    "p(x_3) &= softmax(h^3(x_3)) \\\\\n",
    "        &= \\frac{e^{x_3}}{denom}\n",
    "        &= \\frac{e^2}{denom}\n",
    "        = \\frac{e^2}{1 + e^4 + e^5}\n",
    "        = 0.26538792\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore, $p(x) = \\begin{bmatrix} \n",
    "                        0.01321289 \\\\ \n",
    "                        0.7213992 \\\\ \n",
    "                        0.26538792\n",
    "                        \\end{bmatrix} ≈ \\begin{bmatrix} \n",
    "                        0.0132 \\\\ \n",
    "                        0.7214 \\\\ \n",
    "                        0.2654\n",
    "                        \\end{bmatrix}$\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953b4d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.01321289, 0.7213992 , 0.26538792], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(values):\n",
    "    denom = sum([e**value.numpy() for value in values])\n",
    "    return tf.constant([e**(num.numpy()) / denom for num in values], dtype=float)\n",
    "\n",
    "p = softmax(tf.constant([-1, 3, 2]))\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb780ae",
   "metadata": {},
   "source": [
    "**(e)** What is the predicted class? Is it a correct or incorrect prediction?\n",
    "\n",
    "The predicted class is $2$ as is has the highest probability. It is an incorrect prediction input has a ground truth label of $y = 3 ≠ 2$. \n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e95cb",
   "metadata": {},
   "source": [
    "**(f)** What is the cross-entropy loss caused by the feed-forward neural network at $(x,y)$?\n",
    "\n",
    "$$\\begin{aligned}\n",
    "x &= \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}^T \\\\\n",
    "y &= \\begin{bmatrix} 0.0132 & 0.7214 & 0.2654 \\end{bmatrix}^T \\\\\n",
    "\\\\\n",
    "CE(x, y) &= -\\sum_{m = 1}^{M} x_m \\log y_m \\\\\n",
    "        &= -\\left[x_1 \\log y_1 + x_2 \\log y_2 + x_3 \\log y_3\\right] \\\\\n",
    "        &= -\\left[0 \\log 0.0132 + 0 \\log 0.7214 + 1 \\log 0.2654\\right] \\\\\n",
    "        &= - \\log 0.2654 \\\\\n",
    "        &= 1.3265172 \\\\\n",
    "        &≈ 1.3265\n",
    "\\end{aligned}$$\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d31517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3265172"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([0, 0 ,1], dtype=float)\n",
    "y = tf.constant([0.0132, 0.7214, 0.2654], dtype=float)\n",
    "\n",
    "def ce_loss(x, y):\n",
    "    return sum([-x[i] * log(y[i]) for i in range(x.shape[0])])\n",
    "\n",
    "cs_x_y = ce_loss(x, y)\n",
    "cs_x_y.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483b37a-ce1e-4974-9e97-e81306a81e0b",
   "metadata": {},
   "source": [
    "**(g)** Assume that we are applying the label smoothing technique (i.e.,  [link for main paper](https://papers.nips.cc/paper/2019/file/f1748d6b0fd9d439f71450117eba2725-Paper.pdf) from Goeff Hinton) with $\\alpha = 0.1$. What is the relevant loss caused by the feed-forward neural network at $(x,y)$?\n",
    "\n",
    "In the context of this question, our K value is 3 as we have 3 classes (distinct possible true values).\n",
    "\n",
    "$$\n",
    "y^{LS}_{k} = y_{k}(1 - \\alpha) + \\frac{\\alpha}{K} \\\\[2mm]\n",
    "y^{LS} = y(1 - \\alpha) + \\frac{\\alpha}{K} (I) \\quad \\text{This formula is used instead when considering all }y_k\\text{ values} \\\\[2mm]\n",
    "\\text{where } y_k \\text{ represents the true values for an input } x \\\\\n",
    "\\quad \\text{and } I \\text{ represents an identity vector of size 1 x K}\n",
    "$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y &= \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}^T \\\\\n",
    "\\alpha &= 0.1 \\quad (given) \\\\\n",
    "\\\\\n",
    "y^{LS} &= y(1 - \\alpha) + \\frac{\\alpha}{K} (I) \\\\\n",
    "        &= \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}^T (1 - 0.1) + \\frac{0.1}{3} \\left(\\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}^T\\right) \\\\\n",
    "        &= 0.9 \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}^T + \\frac{1}{30} \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}^T \\\\\n",
    "        &= \\frac{27}{30} \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}^T + \\frac{1}{30} \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}^T \\\\\n",
    "        &= \\frac{1}{30} \\left(27\\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}^T + \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}^T\\right) \\\\\n",
    "        &= \\frac{1}{30} \\left(\\begin{bmatrix} 0 & 0 & 27 \\end{bmatrix}^T + \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}^T\\right) \\\\\n",
    "        &= \\frac{1}{30} \\left(\\begin{bmatrix} 1 & 1 & 28 \\end{bmatrix}^T\\right) \\\\\n",
    "        &= \\begin{bmatrix} \\frac{1}{30} & \\frac{1}{30} & \\frac{28}{30} \\end{bmatrix}^T \\\\\n",
    "        &= \\begin{bmatrix} 0.0\\dot{3} & 0.0\\dot{3} & 0.9\\dot{3} \\end{bmatrix}^T \\\\\n",
    "\\\\\n",
    "y &= \\begin{bmatrix} 0.0049 & 0.7275 & 0.2676 \\end{bmatrix}^T \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore, the relevant loss caused by the feed-forward neural network at $(x,y)$ after applying the label smoothing technique is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "CE(y^{LS}, y) &= -\\sum_{m = 1}^{M} y^{LS}_m \\log y_m \\\\\n",
    "        &= -0.0\\dot{3} \\log{0.0132} - 0.0\\dot{3} \\log{0.7214} - 0.9\\dot{3} \\log{0.2654} \\\\\n",
    "        &= 1.3932195 \\\\\n",
    "        &\\approx 1.3932\n",
    "\\end{aligned}$$\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 point]</span></div> \n",
    "\n",
    "\n",
    "**You need to show both formulas and numerical results for earning full mark. Although it is optional, it is great if you show your numpy code for your computation.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b150fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3932195"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "truth_labels = tf.constant([0, 0, 1], dtype = float)\n",
    "\n",
    "def modify_targets(labels = truth_labels, alpha = 0.1):\n",
    "    return tf.add(tf.multiply(labels, (1 - alpha)), tf.fill(dims = 3, value = alpha / truth_labels.shape[0]))\n",
    "\n",
    "modified_labels = modify_targets()\n",
    "predicted_values = tf.constant([0.0132, 0.7214, 0.2654])\n",
    "\n",
    "new_CE_loss = ce_loss(modified_labels, predicted_values)\n",
    "new_CE_loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06086408-96f0-4871-8827-0e3e2a5d0937",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#0b486b\">Question 1.3: Back propagation</span>\n",
    "Assume that we are constructing a multilayered feed-forward neural network for a classification problem with three classes where the model parameters will be generated randomly using your student ID. \n",
    "\n",
    "The architecture is as follows: $3 (Input)\\rightarrow4(LeakyReLU)\\rightarrow 3(Output)$ (see figure below). Also note that the model parameters are randomly generated.\n",
    "\n",
    "<img src=\"Figures/Q3_P1_S2_2023.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "We then feed a feature vector $x=\\left[\\begin{array}{ccc} 1 & -1 & 0\\end{array}\\right]^{T}$ with ground-truth label $y=3$ to the above network.\n",
    "\n",
    "**You need to show both formulas, numerical results, and your numpy code for your computation for earning full marks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a18b7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_ID = 32809824\n",
    "\n",
    "\"\"\" A Random Number Generator class.\n",
    "It has 1 attribute which acts as the generator of values. \n",
    "The values are generated using the input seed provided in the constructor of the class. \n",
    "If no seed is provided, the default seed of 0 will be used instead. \n",
    "\"\"\"\n",
    "class RNG:\n",
    "    def __init__(self, seed=0):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def next_int(self):\n",
    "        return int(self.rng.random() * 5 // 1 - 2)\n",
    "\n",
    "    def next(self):\n",
    "        return self.rng.random()\n",
    "\n",
    "\"\"\" Simple class to store the dimensions of a 2-Dimensional instance\n",
    "This class is used to store the shape of some 2D Matrix. \n",
    "In the context of this question, it is used to store the shape of the Weights. \n",
    "Attributes:\n",
    "    x - int: The length of the row of the matrix\n",
    "    y - int: The length of the height of the matrix\n",
    "\"\"\"\n",
    "class Size:\n",
    "    def __init__(self, x, y):\n",
    "        self.x, self.y = x, y\n",
    "\n",
    "my_rng = RNG(STUDENT_ID)\n",
    "\n",
    "def populate(size):\n",
    "    return tf.constant([[my_rng.next_int() for _ in range(size.x)] for _ in range(size.y)], dtype=float)\n",
    "\n",
    "# Input value and corresponding label\n",
    "x, y = tf.constant([[1], [-1], [0]], dtype=float), 3\n",
    "\n",
    "# Weight matrices\n",
    "w_dimensions = [None, Size(3, 4), Size(4, 3)]\n",
    "\n",
    "w_values = [\n",
    "    None,\n",
    "    populate(w_dimensions[1]),\n",
    "    populate(w_dimensions[2])\n",
    "]\n",
    "\n",
    "# Bias vectors\n",
    "b_dimensions = [None, Size(1, 4), Size(1, 3)]\n",
    "\n",
    "b_values = [\n",
    "    None,\n",
    "    populate(b_dimensions[1]),\n",
    "    populate(b_dimensions[2])\n",
    "]\n",
    "\n",
    "LEAKY_RELU_PARAMETER = 0.1\n",
    "\n",
    "def leaky_ReLU(num):\n",
    "    return (num if num > 0 else num * LEAKY_RELU_PARAMETER).numpy()\n",
    "\n",
    "def leaky_ReLU_matrix(mat):\n",
    "    shape = mat.shape\n",
    "    return tf.constant([[leaky_ReLU(mat[y][x]) for x in range(shape[1])] for y in range(shape[0])])\n",
    "\n",
    "def leaky_ReLU_deriv(num):\n",
    "    return 1 if num > 0 else LEAKY_RELU_PARAMETER\n",
    "\n",
    "def leaky_ReLU_deriv_matrix(mat):\n",
    "    shape = mat.shape\n",
    "    return tf.constant([[leaky_ReLU_deriv(mat[y][x]) for x in range(shape[1])] for y in range(shape[0])])\n",
    "\n",
    "def diag(vect):\n",
    "    return tf.constant([[vect[index][0].numpy() if index == i else 0 for index in range(vect.shape[0])] for i in range(vect.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c3823",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "LeakyReLU(x) &= \\begin{cases} \n",
    "                x & \\text{if } x > 0 \\\\\n",
    "                ax & \\text{everywhere else } (a \\text{ is some parameter value where } a < 0)\n",
    "                \\end{cases} \\\\\n",
    "\\\\\n",
    "LeakyReLU'(x) &= \\begin{cases} \n",
    "                1\\  & \\text{if } x > 0 \\\\\n",
    "                a\\  & \\text{everywhere else } (a \\text{ is some parameter value where } a < 0)\n",
    "                \\end{cases} \\\\\n",
    "\\\\\n",
    "\\bar{h}_{1}(x) &= W_{1}h_{0}(x) + b_{1} \\\\\n",
    "                &= \\begin{bmatrix} \n",
    "                -1 & -1 & 1 \\\\\n",
    "                -1 & 1 & 1 \\\\\n",
    "                -1 & 0 & 1 \\\\\n",
    "                0 & -1 & -2\n",
    "                \\end{bmatrix}\n",
    "                \\begin{bmatrix} \n",
    "                1 \\\\ -1 \\\\ 0\n",
    "                \\end{bmatrix}\n",
    "                + \\begin{bmatrix} \n",
    "                -1 \\\\ -1 \\\\ 2 \\\\ -2\n",
    "                \\end{bmatrix} \\\\\n",
    "                &= \\begin{bmatrix} 0 \\\\ -2 \\\\ -1 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} -1 \\\\ -1 \\\\ 2 \\\\ -2 \\end{bmatrix} \n",
    "                = \\begin{bmatrix} -1 \\\\ -3 \\\\ 1 \\\\ -1 \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "h_{1}(x) &= \\sigma(\\bar{h}_{1}(x)) \\\\\n",
    "        &= LeakyReLU\\left(\\begin{bmatrix} -1 \\\\ -3 \\\\ 1 \\\\ -1 \\end{bmatrix}\\right)\n",
    "        = \\begin{bmatrix} -0.1 \\\\ -0.3 \\\\ 1 \\\\ -0.1 \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "h_{2}(x) &= W_{2}h_{1}(x) + b_{2} \\\\\n",
    "                &= \\begin{bmatrix} \n",
    "                2 & 0 & 2 & 1 \\\\\n",
    "                -2 & -2 & -1 & 0 \\\\\n",
    "                0 & -2 & -1 & -2\n",
    "                \\end{bmatrix} \n",
    "                \\begin{bmatrix} -0.1 \\\\ -0.3 \\\\ 1 \\\\ -0.1 \\end{bmatrix}\n",
    "                + \\begin{bmatrix} 1 \\\\ 2 \\\\ 1\\end{bmatrix} \\\\\n",
    "                &= \\begin{bmatrix} 1.6\\dot{9} \\\\ -0.01\\dot{9} \\\\ -0.01\\dot{9} \\end{bmatrix}\n",
    "                + \\begin{bmatrix} 1 \\\\ 2 \\\\ 1\\end{bmatrix}\n",
    "                = \\begin{bmatrix} 2.6\\dot{9} \\\\ 1.8 \\\\ 0.8 \\end{bmatrix} \\\\\n",
    "\\\\\n",
    "p(x) &= softmax\\left(h_2(x)\\right) \\\\\n",
    "        &= softmax\\left(\\begin{bmatrix} 2.6\\dot{9} \\\\ 1.8 \\\\ 0.8 \\end{bmatrix}\\right) \\\\\n",
    "        &= \\begin{bmatrix} 0.6426164 \\\\ 0.26126838 \\\\ 0.09611527 \\end{bmatrix}\n",
    "        \\approx \\begin{bmatrix} 0.6426 \\\\ 0.2612 \\\\ 0.0961 \\end{bmatrix} \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d57e1bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.]\n",
      " [-2.]\n",
      " [-1.]\n",
      " [ 1.]], shape=(4, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.]\n",
      " [-3.]\n",
      " [ 1.]\n",
      " [-1.]], shape=(4, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.1]\n",
      " [-0.3]\n",
      " [ 1. ]\n",
      " [-0.1]], shape=(4, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.6999999 ]\n",
      " [-0.19999999]\n",
      " [-0.19999999]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.6999998]\n",
      " [1.8      ]\n",
      " [0.8      ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.6999998]\n",
      " [1.8      ]\n",
      " [0.8      ]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.6426164 ]\n",
      " [0.26126838]\n",
      " [0.09611527]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "h_bar_1 = tf.add(tf.matmul(w_values[1], x), b_values[1])\n",
    "h_1 = leaky_ReLU_matrix(h_bar_1)\n",
    "\n",
    "h_bar_2 = tf.add(tf.matmul(w_values[2], h_1), b_values[2])\n",
    "h_2 = leaky_ReLU_matrix(h_bar_2)\n",
    "\n",
    "p = softmax(h_2)\n",
    "\n",
    "matrices = [tf.matmul(w_values[1], x), h_bar_1, h_1, tf.matmul(w_values[2], h_1), h_bar_2, h_2, p]\n",
    "for m in matrices:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc11ca",
   "metadata": {},
   "source": [
    "**(a)** Suppose that we use the cross-entropy (CE) loss. What is the value of the CE loss $l$?\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y &= [0, 0, 1] \\\\[2mm]\n",
    "CE &= -y_3\\log{p_3} \\\\\n",
    "    &= -\\log{0.2688} \\\\\n",
    "    &= 1.3137876703331426 \\\\\n",
    "    &\\approx 1.3138\n",
    "\n",
    "\\end{aligned}$$\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 point]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1df4ac8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3137876703331426"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CE = -log(0.2688)\n",
    "CE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4289c761",
   "metadata": {},
   "source": [
    "Let $n = 2$ as there are only 3 layers in the neural network above. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "l(x, y; \\theta) &= -\\log \\frac{\\exp\\,\\{h^n_y(x)\\}}  {\\sum_{m = 1}^{M} \\exp\\,\\{h^n_m(x)\\}} \n",
    "                = -\\log \\frac{\\exp\\,\\{h^2_y(x)\\}}  {\\sum_{m = 1}^{M} \\exp\\,\\{h^2_m(x)\\}} \\\\[5mm]\n",
    "                &= -\\left(\\log \\exp\\,\\{h^2_y(x)\\} - \\log \\left[\\sum\\limits_{m = 1}^{M} \\exp\\,\\{h^2_m(x)\\}\\right]\\right) \\\\[5mm]\n",
    "                &= \\log\\left[\\sum\\limits_{m = 1}^{M} \\exp\\,\\{h^2_m(x)\\}\\right] - \\log \\exp\\,\\{h^2_y(x)\\} \\\\\n",
    "                &= \\log\\left[\\sum\\limits_{m = 1}^{M} \\exp\\,\\{h^2_m(x)\\}\\right] - h^2_y(x) \\\\\n",
    "                &= \\log\\left[\\sum\\limits_{m = 1}^{M} \\exp\\,\\{h^2_m(x)\\}\\right] - \\sum\\limits_{m = 1}^{M} 1_{m = y}h^2_m\n",
    "\\end{aligned}$$\n",
    "\n",
    "**(b)** What are the derivatives $\\frac{\\partial l}{\\partial h^{2}},\\frac{\\partial l}{\\partial W^{2}}$, and $\\frac{\\partial l}{\\partial b^{2}}$\n",
    "\n",
    "Expressions:\n",
    "\n",
    "$$\n",
    "h^2 = W^2h^1 + b^2\\quad \\Big|\\quad \\frac{\\partial h^2}{\\partial W^2} = h^1\\quad \\Big|\\quad \\frac{\\partial h^2}{\\partial b^2} = 1\n",
    "$$\n",
    "\n",
    "Derivatives:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial h^2} &= \\frac{\\partial}{\\partial h^2} \\left(\\log\\left[\\sum\\limits_{m = 1}^{M} \\exp\\,\\{h^2_m(x)\\}\\right] - \\sum\\limits_{m = 1}^{M} 1_{m = y}h^2_m\\right) \\\\\n",
    "                                &= \\frac{\\exp\\,\\{h^2_k(x)\\}}{\\sum_{m = 1}^{M} \\exp\\,\\{h^2_m(x)\\}} - \\sum\\limits_{m = 1}^{M} 1_{m = y} \\\\\n",
    "                                &= \\frac{\\exp\\,\\{h^2_k(x)\\}}{\\sum_{m = 1}^{M} \\exp\\,\\{h^2_m(x)\\}} - 1_{m = y} \\\\\n",
    "                                &= softmax(h^2) - 1_y \\\\\n",
    "                                &= p^T - 1_y \\\\\n",
    "                                &= \\begin{bmatrix} 0.0027 & 0.7291 & 0.2688 \\end{bmatrix} - \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\\\\n",
    "                                &= \\begin{bmatrix} 0.0027 & 0.7291 & -0.7312 \\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[3 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a61ebf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 0.00272309,  0.72906786, -0.7317909 ]], dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = tf.constant([0, 0, 1], dtype=float)\n",
    "\n",
    "dl_dh2 = tf.subtract(tf.transpose(p), one_hot)\n",
    "dl_dh2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e119cf",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial W^2} &= \\frac{\\partial l}{\\partial h^2}.\\frac{\\partial h^2}{\\partial W^2} \\\\\n",
    "                                &= (p - 1_y)^Th^{1T} \\\\\n",
    "                                &= \\begin{bmatrix} 0.0027 \\\\ 0.7291 \\\\ -0.7312 \\end{bmatrix}\n",
    "                                \\begin{bmatrix} -0.69 & -2.07 & 1 & -0.69 \\end{bmatrix} \\\\\n",
    "                                &= \\begin{bmatrix} \n",
    "                                -0.0019 & -0.0056 & 0.0027 & -0.0019 \\\\\n",
    "                                -0.5031 & -1.5092 & 0.7291 & -0.5031 \\\\\n",
    "                                0.5049 & 1.5148 & -0.7317909 & 0.5049\n",
    "                                \\end{bmatrix}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca2f701f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[-0.00187893, -0.00563679,  0.00272309, -0.00187893],\n",
       "       [-0.5030568 , -1.5091704 ,  0.72906786, -0.5030568 ],\n",
       "       [ 0.50493574,  1.5148071 , -0.7317909 ,  0.50493574]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_dW2 = tf.matmul(tf.transpose(dl_dh2), tf.transpose(h_1))\n",
    "dl_dW2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec421c",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial b^2} &= \\frac{\\partial l}{\\partial h^2}.\\frac{\\partial h^2}{\\partial b^2} \\\\ \n",
    "                                &= (p - 1_y).1 \\\\\n",
    "                                &= p - 1_y \\\\\n",
    "                                &= \\begin{bmatrix} 0.0027 & 0.7291 & -0.7312 \\end{bmatrix}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d78ae",
   "metadata": {},
   "source": [
    "**(c)** What are the derivatives $\\frac{\\partial l}{\\partial h^{1}}, \\frac{\\partial l}{\\partial \\bar{h}^{1}},\\frac{\\partial l}{\\partial W^{1}}$, and $\\frac{\\partial l}{\\partial b^{1}}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11066db",
   "metadata": {},
   "source": [
    "Expressions:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "h^2 &= W^2h^1 + b^2 \\\\[2mm]\n",
    "\\frac{\\partial h^2}{\\partial h^1} &= W^2 \n",
    "                                = \\begin{bmatrix}\n",
    "                                2 & 0 & 2 & 1 \\\\ \n",
    "                                -2 & -2 & -1 & 0 \\\\\n",
    "                                0 & -2 & -1 & -2\n",
    "                                \\end{bmatrix}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45383e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[ 2.,  0.,  2.,  1.],\n",
       "       [-2., -2., -1.,  0.],\n",
       "       [ 0., -2., -1., -2.]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_values[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d11e3ca",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\bar{h}^1 &= W^1h^0 + b^1 \\\\[1mm]\n",
    "\\frac{\\partial \\bar{h}^1}{\\partial W^1} = h^0 = x &= \\begin{bmatrix} 1 & -1 & 0 \\end{bmatrix}^T\\\\[2mm]\n",
    "\\frac{\\partial \\bar{h}^1}{\\partial h^0} = W^1 &= \\begin{bmatrix} -1 & -1 & 1 \\\\\n",
    "                                                                -1 & 1 & 0 \\\\\n",
    "                                                                -1 & 0 & 1 \\\\\n",
    "                                                                0 & -1 & -2\n",
    "                                                                \\end{bmatrix}\\\\[2mm]\n",
    "\\frac{\\partial \\bar{h}^1}{\\partial b^1} &= 1\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae19f459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[-1., -1.,  1.],\n",
       "       [-1.,  1.,  0.],\n",
       "       [-1.,  0.,  1.],\n",
       "       [ 0., -1., -2.]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b723da7",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "h^1 &= \\sigma(\\bar{h}^1) \\\\[1mm]\n",
    "\\frac{\\partial h^1}{\\partial \\bar{h}^1} &= diag(\\sigma'(\\bar{h}^1))\n",
    "    = diag(LeakyReLU'(\\bar{h}^1)) \\\\\n",
    "    &= diag\\left(LeakyReLU'\\left(\\begin{bmatrix} -1 \\\\ -3 \\\\ 1 \\\\ -1 \\end{bmatrix}\\right)\\right) \\\\\n",
    "    &= diag\\left(\\begin{bmatrix} 0.69 \\\\ 0.69 \\\\ 1 \\\\ 0.69 \\end{bmatrix}\\right)\n",
    "    = \\begin{bmatrix} \n",
    "        0.69 & 0 & 0 & 0 \\\\ \n",
    "        0 & 0.69 & 0 & 0 \\\\\n",
    "        0 & 0 & 1 & 0 \\\\ \n",
    "        0 & 0 & 0 & 0.69 \\end{bmatrix}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7472a02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[0.69, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.69, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.69]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh1_dh_bar1 = diag(leaky_ReLU_deriv_matrix(h_bar_1))\n",
    "dh1_dh_bar1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73596fd3",
   "metadata": {},
   "source": [
    "Derivatives:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial h^1} &= \\frac{\\partial l}{\\partial h^{2}} . \\frac{\\partial h^2}{\\partial h^{1}} \\\\\n",
    "                                &= (p - 1_y)W^2 \\\\\n",
    "                                &= \\begin{bmatrix} 0.0027 & 0.7291 & -0.7312 \\end{bmatrix}\n",
    "                                    \\begin{bmatrix} 2 & 0 & 2 & 1 \\\\ \n",
    "                                                    -2 & -2 & -1 & 0 \\\\\n",
    "                                                    0 & -2 & -1 & -2 \\end{bmatrix} \\\\\n",
    "                                &= \\begin{bmatrix} -1.4527 & 0.0545 & 0.0082 & 1.4663 \\end{bmatrix} \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecb422e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       "array([[-1.4526895 ,  0.00544608,  0.00816923,  1.4663049 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_dh1 = tf.matmul(dl_dh2, w_values[2])\n",
    "dl_dh1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aff7b3",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial \\bar{h}^1} &= \\frac{\\partial l}{\\partial h^1} . \\frac{\\partial h^1}{\\partial \\bar{h}^1} \\\\\n",
    "                                    &= (p - 1_y)W^2 diag(\\sigma'(\\bar{h}^1))\\\\\n",
    "                                    &= \\begin{bmatrix} -1.4527 & 0.0545 & 0.0082 & 1.4663 \\end{bmatrix}\n",
    "                                    \\begin{bmatrix} \n",
    "                                    0.69 & 0 & 0 & 0 \\\\ \n",
    "                                    0 & 0.69 & 0 & 0 \\\\\n",
    "                                    0 & 0 & 1 & 0 \\\\ \n",
    "                                    0 & 0 & 0 & 0.69 \\end{bmatrix} \\\\\n",
    "                                    &= \\begin{bmatrix} -1.0024 & 0.0038 & 0.0082 & 1.0118 \\end{bmatrix} \\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ea7901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       "array([[-1.0023558 ,  0.00375779,  0.00816923,  1.0117503 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_dh_bar1 = tf.matmul(dl_dh1, dh1_dh_bar1)\n",
    "dl_dh_bar1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2122302f",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial W^1} &= \\frac{\\partial l}{\\partial \\bar{h}^1}.\\frac{\\partial \\bar{h}^1}{\\partial W^1} \\\\\n",
    "                                &= \\left((p - 1_y)W^2 diag(\\sigma'(\\bar{h}^1))\\right)^T h^{0T} \\\\\n",
    "                                &= \\begin{bmatrix} -1.0024 \\\\ 0.0038 \\\\ 0.0082 \\\\ 1.0118 \\end{bmatrix}\n",
    "                                \\begin{bmatrix} 1 & -1 & 0 \\end{bmatrix} \\\\\n",
    "                                &= \\begin{bmatrix}\n",
    "                                -1.0024 & 1.0024 & 0 \\\\\n",
    "                                0.0038 & -0.0038 & 0 \\\\\n",
    "                                0.0082 & -0.0082 & 0 \\\\\n",
    "                                1.0118 & -1.0118 & 0 \n",
    "                                \\end{bmatrix}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "843e73a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[-1.0023558 ,  1.0023558 ,  0.        ],\n",
       "       [ 0.00375779, -0.00375779,  0.        ],\n",
       "       [ 0.00816923, -0.00816923,  0.        ],\n",
       "       [ 1.0117503 , -1.0117503 ,  0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl_dW1 = tf.matmul(tf.transpose(dl_dh_bar1), tf.transpose(x))\n",
    "dl_dW1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88816d4a",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "\\frac{\\partial l}{\\partial b^1} &= \\frac{\\partial l}{\\partial \\bar{h}^1}.\\frac{\\partial \\bar{h}^1}{\\partial b^1} \\\\\n",
    "                                &= (p - 1_y)W^2 diag(\\sigma'(\\bar{h}^1)).1 \\\\\n",
    "                                &= (p - 1_y)W^2 diag(\\sigma'(\\bar{h}^1)) \\\\\n",
    "                                &= \\begin{bmatrix} -1.0024 & 0.0038 & 0.0082 & 1.0118 \\end{bmatrix} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[3 points]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a896f1",
   "metadata": {},
   "source": [
    "**(d)** Assume that we use SGD with learning rate $\\eta=0.01$ to update the model parameters. What are the values of $W^2, b^2$ and $W^1, b^1$ after updating?\n",
    "\n",
    "$$\\theta_{n + 1} = \\theta_n - \\eta\\left(\\nabla_{\\theta}J(\\theta_n)\\right)$$\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5a6f6-0a8f-498d-ad06-a2d1063d214f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#0b486b\">Question 1.4: Optimization with gradient descent</span>\n",
    "In this question, we will take a step further to get deeper understanding about gradient descent, one of the most important optimization techniques in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c2297-0d25-4849-9776-2359778ef39b",
   "metadata": {},
   "source": [
    "## **Part (a)**\n",
    "**Question:** Write the pseudo-code to implement the gradient descent algorithm and explain in your words what each line of the code does.\n",
    "\n",
    "**Answer:** $\\texttt{Algorithm}$ <br>\n",
    "Input: objective function 𝑱(𝜽)<br>\n",
    "Output: optimal solution 𝜽<br>\n",
    "\n",
    "Pseudo-code <br>\n",
    "Initialize parameters 𝜃0 randomly ~𝑁(0, 𝜎^2)<br>\n",
    "for 𝑡 = 1 to 𝑇 {<br>\n",
    "Compute gradients g<br>\n",
    "    Update 𝜃(𝑡+1) = 𝜃𝑡 − 𝜂g <br>\n",
    "}<br>\n",
    "Return 𝜃 = 𝜃𝑇+1\n",
    "\n",
    "Inside the for loop, we compute the gradient g which is the rate of change of loss with respect with \n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 point]</span></div>\n",
    "\n",
    "## **Part (b)**\n",
    "**Question:** Explain in your own words why we should update the parameters in the opposite direction of the gradient.\n",
    "\n",
    "**Answer:** We should update the parameters in the opposite direction of the gradient as we wish to approach and eventually reach the minimum possible loss value. The minimum possible loss occurs when our gradient is 0. Therefore, whatever value our gradient is, we wish to update our parameters in the direction that will lead to our gradient to approach 0. This is only possible when we go against the gradient value (direction).\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 point]</span></div>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
